{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FromScratchMNISTclassify.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPI80tBSG7nnV2K/QqCQQtc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fidem/PortfolioProjects/blob/main/FromScratchMNISTclassify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkojU8CPTnyF"
      },
      "source": [
        "#This is my attempt at implementing a Neural network from scratch.  I just want to implement a mirror of my tensorflow dense neural network solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nafcXc8TivH"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPXtBNFT19X"
      },
      "source": [
        "Okay I do want to cheat a bit and get the mnist dataset from the tensorflow keras api, its a lot easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyxt0vtT9R5"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N45NVBtcUFDl"
      },
      "source": [
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihw2tQQVVnVq"
      },
      "source": [
        "Okay so we have our data.  Next is just creating a function for the log softmax activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCIq46OUi01"
      },
      "source": [
        "\n",
        "\n",
        "def logsumexp(x):\n",
        "  a = x.max(axis=1)\n",
        "  return a + np.log(np.exp(x-a.reshape((-1,1))).sum(axis=1))\n",
        "  #return np.log(np.exp(x).sum(axis=1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_H0BCjkAOsF"
      },
      "source": [
        "First we should initialise the network with random weights:\n",
        "\n",
        "So we want to be able to initialise each layer of weights independently, and the weights are randomly sampled from a uniform distribtion between -1 and 1.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on_GYXhT_6Cp"
      },
      "source": [
        "\n",
        "def init_lay(inp, out):\n",
        "  \n",
        "  w = np.random.uniform(-1.,1.,size=(inp,out))/np.sqrt(inp*out)\n",
        "  return w.astype(np.float32)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqpAH8hZcNft"
      },
      "source": [
        "NOTE: OLI THE MATHS FOR THIS IS IN YOUR NOTEBOOK, CANT BE BOTHERED WRITING IT ALL IN MARKDOWN/LATEX...\n",
        "\n",
        "Next is the forward propogation step.  For this, we need to activate the neurons (using weighted sum of inputs like linear regression), then run this through a non-linear activation function, then we just forward propogate to the output layer\n",
        "\n",
        "So first we calc the dot product between our x input and the weights in layer 1.\n",
        "\n",
        "Then we use the relu activation function.  Aka - if value is less than 0, =0, otherwise value = value.  This can be done easily using np.maximum, comparing x_l1, and an array of 0s.\n",
        "\n",
        "\n",
        "NOTE: This is the holy grail code chunk, that has both the forward propogation (Passing input through to the output layer), and the backward propogation (Calculating derivatives of activation functions and layers to get the errors for gradient descent step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhKv4gHpeXbW"
      },
      "source": [
        "def forward_pass(x, y):\n",
        "  out = np.zeros((len(y),10),np.float32)\n",
        "  out[range(out.shape[0]),y] = 1\n",
        "\n",
        "  x_l1 = x.dot(layer1)\n",
        "  x_relu = np.maximum(x_l1,0) \n",
        "  x_l2 = x_relu.dot(layer2)\n",
        "  x_logsum = x_l2 - logsumexp(x_l2).reshape((-1,1))\n",
        "  loss = (-out * x_logsum).mean(axis=1)\n",
        "  d_out = -out / len(y)\n",
        "\n",
        "  return d_out, x_logsum, x_relu, loss, x_l2\n",
        "\n",
        "def backprop(x,y,dout,xlsm,xrelu):\n",
        "  dx_lsm = dout - np.exp(xlsm) * dout.sum(axis=1).reshape((-1,1))\n",
        "  d_l2 = xrelu.T.dot(dx_lsm)\n",
        "  dx_relu = dx_lsm.dot(layer2.T)\n",
        "  dx_l1 = (xrelu > 0).astype(np.float32) * dx_relu\n",
        "  d_l1 = x.T.dot(dx_l1)\n",
        "  return d_l1, d_l2\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjm5cTUVqiHf"
      },
      "source": [
        "Okay that was awful and tragic and weird, but hopefully it will work?  Just need to actually write the loops and include gradient descent on the weights using the d_l1 and d_l2.\n",
        "\n",
        "Need to set the global learning rate variable and initialise the layers.\n",
        "\n",
        "Also idk, with each epoch i dont really really want to train with all 60,000 images.  thats kinda stoopid.  Why not just get a sample of lets say 100 images to train with each epoch??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpKQTh9Fqh3k",
        "outputId": "5e0f8354-622d-4ba8-ac88-ba8214d4e686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "learnrate = 0.001\n",
        "np.random.seed(1)\n",
        "layer1 = init_lay(784,128)\n",
        "layer2 = init_lay(128,10)\n",
        "losses = []\n",
        "accuracy = []\n",
        "\n",
        "for i in range(1000):\n",
        "  rand = np.random.randint(0,x_train.shape[0],size=100)\n",
        "  X = x_train[rand].reshape(-1,28*28)\n",
        "  Y = y_train[rand]\n",
        "  \n",
        "  dout, xlsm, xrelu, loss, x_l2 = forward_pass(X,Y)\n",
        "  losses.append(loss.mean())\n",
        "  dl1, dl2 = backprop(X,Y,dout,xlsm,xrelu)\n",
        "\n",
        "  res = np.argmax(x_l2,axis=1)\n",
        "  acc = (res==Y).mean()\n",
        "  accuracy.append(acc)\n",
        "\n",
        "  layer1 -= learnrate*dl1\n",
        "  layer2 -= learnrate*dl2\n",
        "  print(\"Loss: {}, Accuracy: {} \".format(loss.mean(),acc))\n",
        "  \n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.2800248861312866, Accuracy: 0.09 \n",
            "Loss: 0.337372362613678, Accuracy: 0.29 \n",
            "Loss: 0.30860435962677, Accuracy: 0.48 \n",
            "Loss: 0.44710803031921387, Accuracy: 0.37 \n",
            "Loss: 0.5943931341171265, Accuracy: 0.17 \n",
            "Loss: 0.5106592178344727, Accuracy: 0.23 \n",
            "Loss: 0.2623315155506134, Accuracy: 0.33 \n",
            "Loss: 0.18669414520263672, Accuracy: 0.42 \n",
            "Loss: 0.13514383137226105, Accuracy: 0.57 \n",
            "Loss: 0.11385540664196014, Accuracy: 0.68 \n",
            "Loss: 0.09561576694250107, Accuracy: 0.73 \n",
            "Loss: 0.12429667264223099, Accuracy: 0.59 \n",
            "Loss: 0.10766105353832245, Accuracy: 0.64 \n",
            "Loss: 0.08670919388532639, Accuracy: 0.75 \n",
            "Loss: 0.08082784712314606, Accuracy: 0.74 \n",
            "Loss: 0.06835439056158066, Accuracy: 0.8 \n",
            "Loss: 0.08355072885751724, Accuracy: 0.75 \n",
            "Loss: 0.09786860644817352, Accuracy: 0.7 \n",
            "Loss: 0.07872297614812851, Accuracy: 0.73 \n",
            "Loss: 0.07420124858617783, Accuracy: 0.8 \n",
            "Loss: 0.08655912429094315, Accuracy: 0.71 \n",
            "Loss: 0.061961688101291656, Accuracy: 0.82 \n",
            "Loss: 0.05408738926053047, Accuracy: 0.82 \n",
            "Loss: 0.06899938732385635, Accuracy: 0.78 \n",
            "Loss: 0.08785375952720642, Accuracy: 0.69 \n",
            "Loss: 0.11616745591163635, Accuracy: 0.63 \n",
            "Loss: 0.07921282202005386, Accuracy: 0.78 \n",
            "Loss: 0.06173786148428917, Accuracy: 0.82 \n",
            "Loss: 0.06842508912086487, Accuracy: 0.77 \n",
            "Loss: 0.07173657417297363, Accuracy: 0.83 \n",
            "Loss: 0.03490462899208069, Accuracy: 0.88 \n",
            "Loss: 0.04877784103155136, Accuracy: 0.8 \n",
            "Loss: 0.05443359538912773, Accuracy: 0.84 \n",
            "Loss: 0.0485987514257431, Accuracy: 0.84 \n",
            "Loss: 0.05748176574707031, Accuracy: 0.83 \n",
            "Loss: 0.05792032554745674, Accuracy: 0.8 \n",
            "Loss: 0.04374711215496063, Accuracy: 0.87 \n",
            "Loss: 0.0691583976149559, Accuracy: 0.76 \n",
            "Loss: 0.04115799069404602, Accuracy: 0.92 \n",
            "Loss: 0.04271380603313446, Accuracy: 0.85 \n",
            "Loss: 0.050265468657016754, Accuracy: 0.85 \n",
            "Loss: 0.04599535092711449, Accuracy: 0.87 \n",
            "Loss: 0.05054271221160889, Accuracy: 0.85 \n",
            "Loss: 0.05222659185528755, Accuracy: 0.86 \n",
            "Loss: 0.038927458226680756, Accuracy: 0.89 \n",
            "Loss: 0.04509183019399643, Accuracy: 0.87 \n",
            "Loss: 0.05999644100666046, Accuracy: 0.8 \n",
            "Loss: 0.0668807402253151, Accuracy: 0.81 \n",
            "Loss: 0.04677698016166687, Accuracy: 0.86 \n",
            "Loss: 0.054434634745121, Accuracy: 0.84 \n",
            "Loss: 0.05924868956208229, Accuracy: 0.79 \n",
            "Loss: 0.04015624150633812, Accuracy: 0.85 \n",
            "Loss: 0.03946898505091667, Accuracy: 0.89 \n",
            "Loss: 0.02204490453004837, Accuracy: 0.96 \n",
            "Loss: 0.03715626522898674, Accuracy: 0.9 \n",
            "Loss: 0.03752800077199936, Accuracy: 0.88 \n",
            "Loss: 0.046907976269721985, Accuracy: 0.86 \n",
            "Loss: 0.04123017191886902, Accuracy: 0.88 \n",
            "Loss: 0.0401591882109642, Accuracy: 0.88 \n",
            "Loss: 0.03991673141717911, Accuracy: 0.88 \n",
            "Loss: 0.04533642902970314, Accuracy: 0.88 \n",
            "Loss: 0.028753504157066345, Accuracy: 0.9 \n",
            "Loss: 0.03676208108663559, Accuracy: 0.89 \n",
            "Loss: 0.05638056620955467, Accuracy: 0.84 \n",
            "Loss: 0.03075309284031391, Accuracy: 0.9 \n",
            "Loss: 0.04329526796936989, Accuracy: 0.87 \n",
            "Loss: 0.03642144054174423, Accuracy: 0.87 \n",
            "Loss: 0.049914855509996414, Accuracy: 0.89 \n",
            "Loss: 0.03009602054953575, Accuracy: 0.95 \n",
            "Loss: 0.03600682318210602, Accuracy: 0.87 \n",
            "Loss: 0.030898867174983025, Accuracy: 0.92 \n",
            "Loss: 0.040455762296915054, Accuracy: 0.9 \n",
            "Loss: 0.01925920508801937, Accuracy: 0.95 \n",
            "Loss: 0.05028519779443741, Accuracy: 0.85 \n",
            "Loss: 0.035834163427352905, Accuracy: 0.9 \n",
            "Loss: 0.030978217720985413, Accuracy: 0.9 \n",
            "Loss: 0.027182161808013916, Accuracy: 0.93 \n",
            "Loss: 0.023307792842388153, Accuracy: 0.93 \n",
            "Loss: 0.042249955236911774, Accuracy: 0.89 \n",
            "Loss: 0.04800635948777199, Accuracy: 0.84 \n",
            "Loss: 0.04607871547341347, Accuracy: 0.84 \n",
            "Loss: 0.04992714896798134, Accuracy: 0.83 \n",
            "Loss: 0.02855529636144638, Accuracy: 0.92 \n",
            "Loss: 0.024808192625641823, Accuracy: 0.93 \n",
            "Loss: 0.0286638755351305, Accuracy: 0.93 \n",
            "Loss: 0.03843788430094719, Accuracy: 0.86 \n",
            "Loss: 0.0329459086060524, Accuracy: 0.9 \n",
            "Loss: 0.04210609942674637, Accuracy: 0.84 \n",
            "Loss: 0.03716372326016426, Accuracy: 0.84 \n",
            "Loss: 0.029083251953125, Accuracy: 0.92 \n",
            "Loss: 0.03624275326728821, Accuracy: 0.92 \n",
            "Loss: 0.041850555688142776, Accuracy: 0.85 \n",
            "Loss: 0.040247708559036255, Accuracy: 0.91 \n",
            "Loss: 0.022452404722571373, Accuracy: 0.9 \n",
            "Loss: 0.02386154979467392, Accuracy: 0.92 \n",
            "Loss: 0.03896762430667877, Accuracy: 0.89 \n",
            "Loss: 0.04462014511227608, Accuracy: 0.86 \n",
            "Loss: 0.025119051337242126, Accuracy: 0.93 \n",
            "Loss: 0.05398343876004219, Accuracy: 0.84 \n",
            "Loss: 0.0560753159224987, Accuracy: 0.84 \n",
            "Loss: 0.036318834871053696, Accuracy: 0.91 \n",
            "Loss: 0.03856167569756508, Accuracy: 0.91 \n",
            "Loss: 0.03352697938680649, Accuracy: 0.94 \n",
            "Loss: 0.03192753344774246, Accuracy: 0.89 \n",
            "Loss: 0.024382971227169037, Accuracy: 0.91 \n",
            "Loss: 0.02470283769071102, Accuracy: 0.93 \n",
            "Loss: 0.037098418921232224, Accuracy: 0.9 \n",
            "Loss: 0.02250267192721367, Accuracy: 0.92 \n",
            "Loss: 0.021900271996855736, Accuracy: 0.94 \n",
            "Loss: 0.03715638816356659, Accuracy: 0.9 \n",
            "Loss: 0.031051745638251305, Accuracy: 0.91 \n",
            "Loss: 0.038186199963092804, Accuracy: 0.88 \n",
            "Loss: 0.02208533324301243, Accuracy: 0.94 \n",
            "Loss: 0.018819741904735565, Accuracy: 0.95 \n",
            "Loss: 0.022704876959323883, Accuracy: 0.95 \n",
            "Loss: 0.03808630630373955, Accuracy: 0.9 \n",
            "Loss: 0.02643343061208725, Accuracy: 0.91 \n",
            "Loss: 0.02991555631160736, Accuracy: 0.91 \n",
            "Loss: 0.025151098147034645, Accuracy: 0.94 \n",
            "Loss: 0.01976676657795906, Accuracy: 0.96 \n",
            "Loss: 0.042373158037662506, Accuracy: 0.91 \n",
            "Loss: 0.020059866830706596, Accuracy: 0.94 \n",
            "Loss: 0.023367805406451225, Accuracy: 0.93 \n",
            "Loss: 0.026448708027601242, Accuracy: 0.91 \n",
            "Loss: 0.04896513372659683, Accuracy: 0.87 \n",
            "Loss: 0.049691688269376755, Accuracy: 0.84 \n",
            "Loss: 0.021556895226240158, Accuracy: 0.93 \n",
            "Loss: 0.023737678304314613, Accuracy: 0.91 \n",
            "Loss: 0.015008178539574146, Accuracy: 0.95 \n",
            "Loss: 0.02558765932917595, Accuracy: 0.91 \n",
            "Loss: 0.037291087210178375, Accuracy: 0.91 \n",
            "Loss: 0.030500026419758797, Accuracy: 0.93 \n",
            "Loss: 0.036687981337308884, Accuracy: 0.88 \n",
            "Loss: 0.027680544182658195, Accuracy: 0.93 \n",
            "Loss: 0.03265514224767685, Accuracy: 0.88 \n",
            "Loss: 0.0247043427079916, Accuracy: 0.92 \n",
            "Loss: 0.0363667756319046, Accuracy: 0.89 \n",
            "Loss: 0.02241532877087593, Accuracy: 0.92 \n",
            "Loss: 0.036285243928432465, Accuracy: 0.87 \n",
            "Loss: 0.0461512990295887, Accuracy: 0.83 \n",
            "Loss: 0.0568445548415184, Accuracy: 0.87 \n",
            "Loss: 0.033039823174476624, Accuracy: 0.91 \n",
            "Loss: 0.021300088614225388, Accuracy: 0.93 \n",
            "Loss: 0.02892351895570755, Accuracy: 0.9 \n",
            "Loss: 0.024256112053990364, Accuracy: 0.95 \n",
            "Loss: 0.03129078075289726, Accuracy: 0.88 \n",
            "Loss: 0.0105532705783844, Accuracy: 0.98 \n",
            "Loss: 0.02317912131547928, Accuracy: 0.93 \n",
            "Loss: 0.019622130319476128, Accuracy: 0.94 \n",
            "Loss: 0.02211834117770195, Accuracy: 0.94 \n",
            "Loss: 0.02313603088259697, Accuracy: 0.93 \n",
            "Loss: 0.037156302481889725, Accuracy: 0.9 \n",
            "Loss: 0.039233725517988205, Accuracy: 0.91 \n",
            "Loss: 0.022101186215877533, Accuracy: 0.93 \n",
            "Loss: 0.02627313882112503, Accuracy: 0.94 \n",
            "Loss: 0.026931032538414, Accuracy: 0.94 \n",
            "Loss: 0.026819152757525444, Accuracy: 0.92 \n",
            "Loss: 0.027308227494359016, Accuracy: 0.92 \n",
            "Loss: 0.03202224150300026, Accuracy: 0.93 \n",
            "Loss: 0.025057239457964897, Accuracy: 0.94 \n",
            "Loss: 0.031521026045084, Accuracy: 0.9 \n",
            "Loss: 0.020068194717168808, Accuracy: 0.94 \n",
            "Loss: 0.037688348442316055, Accuracy: 0.88 \n",
            "Loss: 0.034035615622997284, Accuracy: 0.88 \n",
            "Loss: 0.02797851338982582, Accuracy: 0.91 \n",
            "Loss: 0.024963844567537308, Accuracy: 0.93 \n",
            "Loss: 0.031109629198908806, Accuracy: 0.91 \n",
            "Loss: 0.03700683265924454, Accuracy: 0.91 \n",
            "Loss: 0.028490331023931503, Accuracy: 0.94 \n",
            "Loss: 0.018602658063173294, Accuracy: 0.93 \n",
            "Loss: 0.027111606672406197, Accuracy: 0.9 \n",
            "Loss: 0.03793260455131531, Accuracy: 0.91 \n",
            "Loss: 0.034574393182992935, Accuracy: 0.9 \n",
            "Loss: 0.03361134976148605, Accuracy: 0.91 \n",
            "Loss: 0.020803742110729218, Accuracy: 0.95 \n",
            "Loss: 0.03478385508060455, Accuracy: 0.89 \n",
            "Loss: 0.031707633286714554, Accuracy: 0.9 \n",
            "Loss: 0.0175776407122612, Accuracy: 0.96 \n",
            "Loss: 0.02382875606417656, Accuracy: 0.95 \n",
            "Loss: 0.03124573640525341, Accuracy: 0.91 \n",
            "Loss: 0.022485651075839996, Accuracy: 0.92 \n",
            "Loss: 0.026010995730757713, Accuracy: 0.92 \n",
            "Loss: 0.028488948941230774, Accuracy: 0.91 \n",
            "Loss: 0.03962011635303497, Accuracy: 0.87 \n",
            "Loss: 0.026398533955216408, Accuracy: 0.89 \n",
            "Loss: 0.03119266964495182, Accuracy: 0.89 \n",
            "Loss: 0.02649769000709057, Accuracy: 0.93 \n",
            "Loss: 0.04613403230905533, Accuracy: 0.84 \n",
            "Loss: 0.02544364146888256, Accuracy: 0.92 \n",
            "Loss: 0.02699672430753708, Accuracy: 0.91 \n",
            "Loss: 0.014974551275372505, Accuracy: 0.94 \n",
            "Loss: 0.018815644085407257, Accuracy: 0.95 \n",
            "Loss: 0.020310349762439728, Accuracy: 0.92 \n",
            "Loss: 0.013798248954117298, Accuracy: 0.98 \n",
            "Loss: 0.019145453348755836, Accuracy: 0.95 \n",
            "Loss: 0.02117585577070713, Accuracy: 0.94 \n",
            "Loss: 0.022205064073204994, Accuracy: 0.94 \n",
            "Loss: 0.017881520092487335, Accuracy: 0.96 \n",
            "Loss: 0.0246245376765728, Accuracy: 0.92 \n",
            "Loss: 0.035978030413389206, Accuracy: 0.91 \n",
            "Loss: 0.018582085147500038, Accuracy: 0.93 \n",
            "Loss: 0.031614162027835846, Accuracy: 0.94 \n",
            "Loss: 0.022772550582885742, Accuracy: 0.94 \n",
            "Loss: 0.016703998669981956, Accuracy: 0.93 \n",
            "Loss: 0.015846850350499153, Accuracy: 0.95 \n",
            "Loss: 0.022815076634287834, Accuracy: 0.93 \n",
            "Loss: 0.021320907399058342, Accuracy: 0.94 \n",
            "Loss: 0.020016897469758987, Accuracy: 0.94 \n",
            "Loss: 0.02667425200343132, Accuracy: 0.94 \n",
            "Loss: 0.02499326318502426, Accuracy: 0.9 \n",
            "Loss: 0.03120780922472477, Accuracy: 0.9 \n",
            "Loss: 0.036729518324136734, Accuracy: 0.92 \n",
            "Loss: 0.020533908158540726, Accuracy: 0.95 \n",
            "Loss: 0.03213533014059067, Accuracy: 0.91 \n",
            "Loss: 0.029929427430033684, Accuracy: 0.88 \n",
            "Loss: 0.02553863450884819, Accuracy: 0.95 \n",
            "Loss: 0.03201373293995857, Accuracy: 0.92 \n",
            "Loss: 0.021452726796269417, Accuracy: 0.93 \n",
            "Loss: 0.030126500874757767, Accuracy: 0.91 \n",
            "Loss: 0.020860396325588226, Accuracy: 0.95 \n",
            "Loss: 0.020246516913175583, Accuracy: 0.93 \n",
            "Loss: 0.03101385198533535, Accuracy: 0.92 \n",
            "Loss: 0.019132258370518684, Accuracy: 0.93 \n",
            "Loss: 0.01651289314031601, Accuracy: 0.96 \n",
            "Loss: 0.029236676171422005, Accuracy: 0.91 \n",
            "Loss: 0.027251893654465675, Accuracy: 0.92 \n",
            "Loss: 0.014232278801500797, Accuracy: 0.96 \n",
            "Loss: 0.02250094898045063, Accuracy: 0.92 \n",
            "Loss: 0.02853657305240631, Accuracy: 0.88 \n",
            "Loss: 0.019411170855164528, Accuracy: 0.93 \n",
            "Loss: 0.020557107403874397, Accuracy: 0.93 \n",
            "Loss: 0.010843943804502487, Accuracy: 0.96 \n",
            "Loss: 0.03190738335251808, Accuracy: 0.89 \n",
            "Loss: 0.01748126931488514, Accuracy: 0.96 \n",
            "Loss: 0.02820059284567833, Accuracy: 0.91 \n",
            "Loss: 0.018084725365042686, Accuracy: 0.93 \n",
            "Loss: 0.028131764382123947, Accuracy: 0.92 \n",
            "Loss: 0.01674078032374382, Accuracy: 0.96 \n",
            "Loss: 0.02212315984070301, Accuracy: 0.92 \n",
            "Loss: 0.0185924731194973, Accuracy: 0.94 \n",
            "Loss: 0.030889911577105522, Accuracy: 0.9 \n",
            "Loss: 0.01876424252986908, Accuracy: 0.92 \n",
            "Loss: 0.019693098962306976, Accuracy: 0.93 \n",
            "Loss: 0.018160033971071243, Accuracy: 0.94 \n",
            "Loss: 0.03142762556672096, Accuracy: 0.94 \n",
            "Loss: 0.018297938629984856, Accuracy: 0.94 \n",
            "Loss: 0.017941655591130257, Accuracy: 0.95 \n",
            "Loss: 0.01921802945435047, Accuracy: 0.95 \n",
            "Loss: 0.050234418362379074, Accuracy: 0.86 \n",
            "Loss: 0.022806162014603615, Accuracy: 0.92 \n",
            "Loss: 0.019645564258098602, Accuracy: 0.93 \n",
            "Loss: 0.016190797090530396, Accuracy: 0.95 \n",
            "Loss: 0.027377454563975334, Accuracy: 0.92 \n",
            "Loss: 0.017506495118141174, Accuracy: 0.94 \n",
            "Loss: 0.01474775280803442, Accuracy: 0.95 \n",
            "Loss: 0.019198330119252205, Accuracy: 0.95 \n",
            "Loss: 0.018535323441028595, Accuracy: 0.93 \n",
            "Loss: 0.014798568561673164, Accuracy: 0.95 \n",
            "Loss: 0.024379288777709007, Accuracy: 0.94 \n",
            "Loss: 0.015688570216298103, Accuracy: 0.97 \n",
            "Loss: 0.023678645491600037, Accuracy: 0.94 \n",
            "Loss: 0.021144026890397072, Accuracy: 0.95 \n",
            "Loss: 0.017800716683268547, Accuracy: 0.96 \n",
            "Loss: 0.025338687002658844, Accuracy: 0.94 \n",
            "Loss: 0.029869060963392258, Accuracy: 0.88 \n",
            "Loss: 0.02653907798230648, Accuracy: 0.89 \n",
            "Loss: 0.012701001949608326, Accuracy: 0.96 \n",
            "Loss: 0.018552618101239204, Accuracy: 0.93 \n",
            "Loss: 0.028860993683338165, Accuracy: 0.92 \n",
            "Loss: 0.021393589675426483, Accuracy: 0.97 \n",
            "Loss: 0.022503744810819626, Accuracy: 0.9 \n",
            "Loss: 0.025339307263493538, Accuracy: 0.91 \n",
            "Loss: 0.023209255188703537, Accuracy: 0.96 \n",
            "Loss: 0.021893993020057678, Accuracy: 0.93 \n",
            "Loss: 0.017357248812913895, Accuracy: 0.94 \n",
            "Loss: 0.024132218211889267, Accuracy: 0.91 \n",
            "Loss: 0.030848871916532516, Accuracy: 0.94 \n",
            "Loss: 0.0204910971224308, Accuracy: 0.94 \n",
            "Loss: 0.020390713587403297, Accuracy: 0.92 \n",
            "Loss: 0.02233557403087616, Accuracy: 0.92 \n",
            "Loss: 0.013613355346024036, Accuracy: 0.95 \n",
            "Loss: 0.026930436491966248, Accuracy: 0.93 \n",
            "Loss: 0.02602210082113743, Accuracy: 0.94 \n",
            "Loss: 0.012088562361896038, Accuracy: 0.97 \n",
            "Loss: 0.02655598893761635, Accuracy: 0.92 \n",
            "Loss: 0.02335957996547222, Accuracy: 0.93 \n",
            "Loss: 0.023582568392157555, Accuracy: 0.94 \n",
            "Loss: 0.02279343083500862, Accuracy: 0.94 \n",
            "Loss: 0.032604772597551346, Accuracy: 0.91 \n",
            "Loss: 0.014089410193264484, Accuracy: 0.97 \n",
            "Loss: 0.023448210209608078, Accuracy: 0.94 \n",
            "Loss: 0.02442740648984909, Accuracy: 0.89 \n",
            "Loss: 0.02170737273991108, Accuracy: 0.92 \n",
            "Loss: 0.022278616204857826, Accuracy: 0.94 \n",
            "Loss: 0.034348148852586746, Accuracy: 0.93 \n",
            "Loss: 0.017574423924088478, Accuracy: 0.95 \n",
            "Loss: 0.021294651553034782, Accuracy: 0.94 \n",
            "Loss: 0.022760730236768723, Accuracy: 0.93 \n",
            "Loss: 0.024704284965991974, Accuracy: 0.93 \n",
            "Loss: 0.012302923947572708, Accuracy: 0.98 \n",
            "Loss: 0.02657194435596466, Accuracy: 0.94 \n",
            "Loss: 0.019054627045989037, Accuracy: 0.96 \n",
            "Loss: 0.02390565723180771, Accuracy: 0.91 \n",
            "Loss: 0.012087056413292885, Accuracy: 0.97 \n",
            "Loss: 0.018344761803746223, Accuracy: 0.95 \n",
            "Loss: 0.02326939068734646, Accuracy: 0.94 \n",
            "Loss: 0.020392287522554398, Accuracy: 0.93 \n",
            "Loss: 0.020955802872776985, Accuracy: 0.94 \n",
            "Loss: 0.014804418198764324, Accuracy: 0.96 \n",
            "Loss: 0.024213409051299095, Accuracy: 0.92 \n",
            "Loss: 0.017249727621674538, Accuracy: 0.97 \n",
            "Loss: 0.018236396834254265, Accuracy: 0.94 \n",
            "Loss: 0.021960940212011337, Accuracy: 0.92 \n",
            "Loss: 0.008073211647570133, Accuracy: 1.0 \n",
            "Loss: 0.018625464290380478, Accuracy: 0.95 \n",
            "Loss: 0.030164584517478943, Accuracy: 0.92 \n",
            "Loss: 0.02283077873289585, Accuracy: 0.92 \n",
            "Loss: 0.03060140833258629, Accuracy: 0.91 \n",
            "Loss: 0.02285788021981716, Accuracy: 0.95 \n",
            "Loss: 0.02637765184044838, Accuracy: 0.93 \n",
            "Loss: 0.03481672704219818, Accuracy: 0.91 \n",
            "Loss: 0.01924828812479973, Accuracy: 0.94 \n",
            "Loss: 0.020540950819849968, Accuracy: 0.95 \n",
            "Loss: 0.03261392563581467, Accuracy: 0.88 \n",
            "Loss: 0.024963652715086937, Accuracy: 0.92 \n",
            "Loss: 0.031562041491270065, Accuracy: 0.88 \n",
            "Loss: 0.019411293789744377, Accuracy: 0.95 \n",
            "Loss: 0.014834292232990265, Accuracy: 0.95 \n",
            "Loss: 0.018073484301567078, Accuracy: 0.93 \n",
            "Loss: 0.031011948361992836, Accuracy: 0.93 \n",
            "Loss: 0.01866314932703972, Accuracy: 0.95 \n",
            "Loss: 0.017314458265900612, Accuracy: 0.97 \n",
            "Loss: 0.02450447529554367, Accuracy: 0.93 \n",
            "Loss: 0.029110200703144073, Accuracy: 0.92 \n",
            "Loss: 0.023647882044315338, Accuracy: 0.96 \n",
            "Loss: 0.014372793026268482, Accuracy: 0.96 \n",
            "Loss: 0.028757624328136444, Accuracy: 0.91 \n",
            "Loss: 0.026818597689270973, Accuracy: 0.94 \n",
            "Loss: 0.03283011540770531, Accuracy: 0.85 \n",
            "Loss: 0.01867757737636566, Accuracy: 0.96 \n",
            "Loss: 0.016965139657258987, Accuracy: 0.94 \n",
            "Loss: 0.020296817645430565, Accuracy: 0.9 \n",
            "Loss: 0.02695881389081478, Accuracy: 0.93 \n",
            "Loss: 0.023555876687169075, Accuracy: 0.92 \n",
            "Loss: 0.027438761666417122, Accuracy: 0.89 \n",
            "Loss: 0.015382686629891396, Accuracy: 0.94 \n",
            "Loss: 0.024493644014000893, Accuracy: 0.93 \n",
            "Loss: 0.019208671525120735, Accuracy: 0.9 \n",
            "Loss: 0.004951719660311937, Accuracy: 1.0 \n",
            "Loss: 0.01797545701265335, Accuracy: 0.94 \n",
            "Loss: 0.019937925040721893, Accuracy: 0.93 \n",
            "Loss: 0.007517184596508741, Accuracy: 0.98 \n",
            "Loss: 0.02016330324113369, Accuracy: 0.94 \n",
            "Loss: 0.009007508866488934, Accuracy: 0.98 \n",
            "Loss: 0.012822102755308151, Accuracy: 0.96 \n",
            "Loss: 0.01615329086780548, Accuracy: 0.93 \n",
            "Loss: 0.03323988988995552, Accuracy: 0.9 \n",
            "Loss: 0.010093766264617443, Accuracy: 0.99 \n",
            "Loss: 0.01157968770712614, Accuracy: 0.96 \n",
            "Loss: 0.01461813598871231, Accuracy: 0.94 \n",
            "Loss: 0.023378100246191025, Accuracy: 0.9 \n",
            "Loss: 0.028601640835404396, Accuracy: 0.9 \n",
            "Loss: 0.01922861486673355, Accuracy: 0.95 \n",
            "Loss: 0.019243914633989334, Accuracy: 0.93 \n",
            "Loss: 0.0238286592066288, Accuracy: 0.94 \n",
            "Loss: 0.019140658900141716, Accuracy: 0.96 \n",
            "Loss: 0.02936127409338951, Accuracy: 0.92 \n",
            "Loss: 0.014045714400708675, Accuracy: 0.95 \n",
            "Loss: 0.016019560396671295, Accuracy: 0.94 \n",
            "Loss: 0.02375376969575882, Accuracy: 0.89 \n",
            "Loss: 0.021758250892162323, Accuracy: 0.93 \n",
            "Loss: 0.02565125748515129, Accuracy: 0.91 \n",
            "Loss: 0.02604461833834648, Accuracy: 0.93 \n",
            "Loss: 0.022232413291931152, Accuracy: 0.93 \n",
            "Loss: 0.022919610142707825, Accuracy: 0.93 \n",
            "Loss: 0.014355827122926712, Accuracy: 0.95 \n",
            "Loss: 0.02271696738898754, Accuracy: 0.94 \n",
            "Loss: 0.026177966967225075, Accuracy: 0.92 \n",
            "Loss: 0.023998918011784554, Accuracy: 0.92 \n",
            "Loss: 0.02492576092481613, Accuracy: 0.92 \n",
            "Loss: 0.013928284868597984, Accuracy: 0.94 \n",
            "Loss: 0.033803828060626984, Accuracy: 0.91 \n",
            "Loss: 0.01811966300010681, Accuracy: 0.94 \n",
            "Loss: 0.03378690779209137, Accuracy: 0.9 \n",
            "Loss: 0.023508863523602486, Accuracy: 0.93 \n",
            "Loss: 0.01808718964457512, Accuracy: 0.94 \n",
            "Loss: 0.013080542907118797, Accuracy: 0.95 \n",
            "Loss: 0.0189146026968956, Accuracy: 0.94 \n",
            "Loss: 0.028620360419154167, Accuracy: 0.92 \n",
            "Loss: 0.019330058246850967, Accuracy: 0.94 \n",
            "Loss: 0.03802778199315071, Accuracy: 0.88 \n",
            "Loss: 0.010169113986194134, Accuracy: 0.97 \n",
            "Loss: 0.01531411986798048, Accuracy: 0.95 \n",
            "Loss: 0.011444819159805775, Accuracy: 0.96 \n",
            "Loss: 0.03135104104876518, Accuracy: 0.95 \n",
            "Loss: 0.011612823233008385, Accuracy: 0.97 \n",
            "Loss: 0.014386248774826527, Accuracy: 0.97 \n",
            "Loss: 0.02350125089287758, Accuracy: 0.92 \n",
            "Loss: 0.02054879255592823, Accuracy: 0.94 \n",
            "Loss: 0.017040414735674858, Accuracy: 0.94 \n",
            "Loss: 0.01579652912914753, Accuracy: 0.95 \n",
            "Loss: 0.02094651758670807, Accuracy: 0.96 \n",
            "Loss: 0.03687648847699165, Accuracy: 0.9 \n",
            "Loss: 0.03164158761501312, Accuracy: 0.87 \n",
            "Loss: 0.01676781475543976, Accuracy: 0.95 \n",
            "Loss: 0.021866820752620697, Accuracy: 0.93 \n",
            "Loss: 0.016813263297080994, Accuracy: 0.95 \n",
            "Loss: 0.01415107399225235, Accuracy: 0.95 \n",
            "Loss: 0.030355608090758324, Accuracy: 0.94 \n",
            "Loss: 0.01294824481010437, Accuracy: 0.96 \n",
            "Loss: 0.011441064067184925, Accuracy: 0.97 \n",
            "Loss: 0.022032611072063446, Accuracy: 0.93 \n",
            "Loss: 0.01734643056988716, Accuracy: 0.94 \n",
            "Loss: 0.007989420555531979, Accuracy: 0.97 \n",
            "Loss: 0.014607226476073265, Accuracy: 0.94 \n",
            "Loss: 0.019876468926668167, Accuracy: 0.94 \n",
            "Loss: 0.020380988717079163, Accuracy: 0.93 \n",
            "Loss: 0.018720954656600952, Accuracy: 0.93 \n",
            "Loss: 0.02384614758193493, Accuracy: 0.92 \n",
            "Loss: 0.019269438460469246, Accuracy: 0.94 \n",
            "Loss: 0.018623188138008118, Accuracy: 0.93 \n",
            "Loss: 0.020812802016735077, Accuracy: 0.93 \n",
            "Loss: 0.02169601246714592, Accuracy: 0.92 \n",
            "Loss: 0.007707989774644375, Accuracy: 0.99 \n",
            "Loss: 0.013552767224609852, Accuracy: 0.96 \n",
            "Loss: 0.02379651553928852, Accuracy: 0.92 \n",
            "Loss: 0.033312130719423294, Accuracy: 0.91 \n",
            "Loss: 0.020204098895192146, Accuracy: 0.93 \n",
            "Loss: 0.00885983370244503, Accuracy: 0.98 \n",
            "Loss: 0.01924651674926281, Accuracy: 0.93 \n",
            "Loss: 0.016681499779224396, Accuracy: 0.95 \n",
            "Loss: 0.017988281324505806, Accuracy: 0.95 \n",
            "Loss: 0.015081166289746761, Accuracy: 0.95 \n",
            "Loss: 0.024259047582745552, Accuracy: 0.91 \n",
            "Loss: 0.02118598483502865, Accuracy: 0.93 \n",
            "Loss: 0.01642880029976368, Accuracy: 0.98 \n",
            "Loss: 0.014058568514883518, Accuracy: 0.95 \n",
            "Loss: 0.015096716582775116, Accuracy: 0.96 \n",
            "Loss: 0.028744010254740715, Accuracy: 0.9 \n",
            "Loss: 0.02085099369287491, Accuracy: 0.91 \n",
            "Loss: 0.008583463728427887, Accuracy: 0.98 \n",
            "Loss: 0.015605912543833256, Accuracy: 0.96 \n",
            "Loss: 0.014019246213138103, Accuracy: 0.97 \n",
            "Loss: 0.0189618319272995, Accuracy: 0.94 \n",
            "Loss: 0.01795538328588009, Accuracy: 0.95 \n",
            "Loss: 0.017735730856657028, Accuracy: 0.94 \n",
            "Loss: 0.022357989102602005, Accuracy: 0.94 \n",
            "Loss: 0.0139003936201334, Accuracy: 0.97 \n",
            "Loss: 0.03457794338464737, Accuracy: 0.91 \n",
            "Loss: 0.015976272523403168, Accuracy: 0.93 \n",
            "Loss: 0.01735619083046913, Accuracy: 0.94 \n",
            "Loss: 0.03153926879167557, Accuracy: 0.89 \n",
            "Loss: 0.015385312028229237, Accuracy: 0.97 \n",
            "Loss: 0.006628194358199835, Accuracy: 0.99 \n",
            "Loss: 0.028214529156684875, Accuracy: 0.91 \n",
            "Loss: 0.015131025575101376, Accuracy: 0.94 \n",
            "Loss: 0.010068545117974281, Accuracy: 0.97 \n",
            "Loss: 0.013638688251376152, Accuracy: 0.97 \n",
            "Loss: 0.011380749754607677, Accuracy: 0.97 \n",
            "Loss: 0.01374889723956585, Accuracy: 0.96 \n",
            "Loss: 0.014464977197349072, Accuracy: 0.95 \n",
            "Loss: 0.01223256066441536, Accuracy: 0.97 \n",
            "Loss: 0.015134952031075954, Accuracy: 0.97 \n",
            "Loss: 0.014491494745016098, Accuracy: 0.96 \n",
            "Loss: 0.01523530576378107, Accuracy: 0.95 \n",
            "Loss: 0.016290931031107903, Accuracy: 0.95 \n",
            "Loss: 0.01287358719855547, Accuracy: 0.95 \n",
            "Loss: 0.01621379517018795, Accuracy: 0.95 \n",
            "Loss: 0.0106271430850029, Accuracy: 0.99 \n",
            "Loss: 0.012300458736717701, Accuracy: 0.94 \n",
            "Loss: 0.017313461750745773, Accuracy: 0.94 \n",
            "Loss: 0.022132722660899162, Accuracy: 0.94 \n",
            "Loss: 0.023441171273589134, Accuracy: 0.89 \n",
            "Loss: 0.014408200979232788, Accuracy: 0.93 \n",
            "Loss: 0.01632522977888584, Accuracy: 0.95 \n",
            "Loss: 0.01429782621562481, Accuracy: 0.95 \n",
            "Loss: 0.009287308901548386, Accuracy: 0.97 \n",
            "Loss: 0.021983526647090912, Accuracy: 0.96 \n",
            "Loss: 0.014192324131727219, Accuracy: 0.96 \n",
            "Loss: 0.007180875167250633, Accuracy: 0.98 \n",
            "Loss: 0.01328372023999691, Accuracy: 0.97 \n",
            "Loss: 0.023630375042557716, Accuracy: 0.93 \n",
            "Loss: 0.02839565835893154, Accuracy: 0.92 \n",
            "Loss: 0.021747520193457603, Accuracy: 0.94 \n",
            "Loss: 0.012994092889130116, Accuracy: 0.97 \n",
            "Loss: 0.011862244457006454, Accuracy: 0.95 \n",
            "Loss: 0.007989054545760155, Accuracy: 0.98 \n",
            "Loss: 0.020302534103393555, Accuracy: 0.93 \n",
            "Loss: 0.02501864731311798, Accuracy: 0.91 \n",
            "Loss: 0.02488398551940918, Accuracy: 0.93 \n",
            "Loss: 0.009731564670801163, Accuracy: 0.97 \n",
            "Loss: 0.015277893282473087, Accuracy: 0.94 \n",
            "Loss: 0.01978648640215397, Accuracy: 0.94 \n",
            "Loss: 0.009939780458807945, Accuracy: 0.98 \n",
            "Loss: 0.023461254313588142, Accuracy: 0.94 \n",
            "Loss: 0.016143932938575745, Accuracy: 0.95 \n",
            "Loss: 0.015140260569751263, Accuracy: 0.97 \n",
            "Loss: 0.016473542898893356, Accuracy: 0.95 \n",
            "Loss: 0.01875271275639534, Accuracy: 0.95 \n",
            "Loss: 0.009322288446128368, Accuracy: 0.97 \n",
            "Loss: 0.0171983502805233, Accuracy: 0.95 \n",
            "Loss: 0.014141232706606388, Accuracy: 0.96 \n",
            "Loss: 0.015309764072299004, Accuracy: 0.93 \n",
            "Loss: 0.011966227553784847, Accuracy: 0.98 \n",
            "Loss: 0.011603963561356068, Accuracy: 0.95 \n",
            "Loss: 0.013827444054186344, Accuracy: 0.97 \n",
            "Loss: 0.0270844753831625, Accuracy: 0.93 \n",
            "Loss: 0.015495541505515575, Accuracy: 0.96 \n",
            "Loss: 0.011722461320459843, Accuracy: 0.98 \n",
            "Loss: 0.015057818032801151, Accuracy: 0.95 \n",
            "Loss: 0.01718098856508732, Accuracy: 0.95 \n",
            "Loss: 0.015844933688640594, Accuracy: 0.94 \n",
            "Loss: 0.024982569739222527, Accuracy: 0.92 \n",
            "Loss: 0.012699317187070847, Accuracy: 0.96 \n",
            "Loss: 0.03399721533060074, Accuracy: 0.92 \n",
            "Loss: 0.012319193221628666, Accuracy: 0.97 \n",
            "Loss: 0.007080152630805969, Accuracy: 0.98 \n",
            "Loss: 0.011697727255523205, Accuracy: 0.97 \n",
            "Loss: 0.019353346899151802, Accuracy: 0.94 \n",
            "Loss: 0.013846814632415771, Accuracy: 0.95 \n",
            "Loss: 0.027216238901019096, Accuracy: 0.92 \n",
            "Loss: 0.025060007348656654, Accuracy: 0.95 \n",
            "Loss: 0.0183250829577446, Accuracy: 0.96 \n",
            "Loss: 0.010658119805157185, Accuracy: 0.97 \n",
            "Loss: 0.020340755581855774, Accuracy: 0.93 \n",
            "Loss: 0.029442939907312393, Accuracy: 0.91 \n",
            "Loss: 0.008040152490139008, Accuracy: 0.97 \n",
            "Loss: 0.013030653819441795, Accuracy: 0.97 \n",
            "Loss: 0.008141565136611462, Accuracy: 0.99 \n",
            "Loss: 0.015030059032142162, Accuracy: 0.95 \n",
            "Loss: 0.010459140874445438, Accuracy: 0.98 \n",
            "Loss: 0.011585688218474388, Accuracy: 0.96 \n",
            "Loss: 0.013909425586462021, Accuracy: 0.95 \n",
            "Loss: 0.009834034368395805, Accuracy: 0.96 \n",
            "Loss: 0.009476087056100368, Accuracy: 0.97 \n",
            "Loss: 0.01563318446278572, Accuracy: 0.96 \n",
            "Loss: 0.016172807663679123, Accuracy: 0.97 \n",
            "Loss: 0.005721437279134989, Accuracy: 0.99 \n",
            "Loss: 0.014929103665053844, Accuracy: 0.96 \n",
            "Loss: 0.026053542271256447, Accuracy: 0.93 \n",
            "Loss: 0.0174352265894413, Accuracy: 0.96 \n",
            "Loss: 0.02204693853855133, Accuracy: 0.91 \n",
            "Loss: 0.021421588957309723, Accuracy: 0.94 \n",
            "Loss: 0.01361151784658432, Accuracy: 0.96 \n",
            "Loss: 0.008622856810688972, Accuracy: 0.99 \n",
            "Loss: 0.02042311429977417, Accuracy: 0.94 \n",
            "Loss: 0.014495659619569778, Accuracy: 0.94 \n",
            "Loss: 0.0249420627951622, Accuracy: 0.93 \n",
            "Loss: 0.01933172158896923, Accuracy: 0.92 \n",
            "Loss: 0.019147325307130814, Accuracy: 0.95 \n",
            "Loss: 0.013467162847518921, Accuracy: 0.96 \n",
            "Loss: 0.007937624119222164, Accuracy: 0.99 \n",
            "Loss: 0.02374068647623062, Accuracy: 0.91 \n",
            "Loss: 0.01596103236079216, Accuracy: 0.93 \n",
            "Loss: 0.018198547884821892, Accuracy: 0.94 \n",
            "Loss: 0.019224824383854866, Accuracy: 0.96 \n",
            "Loss: 0.029346415773034096, Accuracy: 0.93 \n",
            "Loss: 0.014112298376858234, Accuracy: 0.95 \n",
            "Loss: 0.012582771480083466, Accuracy: 0.96 \n",
            "Loss: 0.01469525694847107, Accuracy: 0.95 \n",
            "Loss: 0.01599537581205368, Accuracy: 0.95 \n",
            "Loss: 0.022013474255800247, Accuracy: 0.87 \n",
            "Loss: 0.015324813313782215, Accuracy: 0.96 \n",
            "Loss: 0.008996497839689255, Accuracy: 0.98 \n",
            "Loss: 0.016354883089661598, Accuracy: 0.93 \n",
            "Loss: 0.01863144338130951, Accuracy: 0.95 \n",
            "Loss: 0.014103194698691368, Accuracy: 0.96 \n",
            "Loss: 0.01482707355171442, Accuracy: 0.96 \n",
            "Loss: 0.01923217624425888, Accuracy: 0.93 \n",
            "Loss: 0.012181536294519901, Accuracy: 0.97 \n",
            "Loss: 0.018202368170022964, Accuracy: 0.93 \n",
            "Loss: 0.022008469328284264, Accuracy: 0.94 \n",
            "Loss: 0.011265477165579796, Accuracy: 0.98 \n",
            "Loss: 0.017558949068188667, Accuracy: 0.96 \n",
            "Loss: 0.012224911712110043, Accuracy: 0.96 \n",
            "Loss: 0.013980682939291, Accuracy: 0.97 \n",
            "Loss: 0.011342156678438187, Accuracy: 0.98 \n",
            "Loss: 0.024764657020568848, Accuracy: 0.96 \n",
            "Loss: 0.013050156645476818, Accuracy: 0.97 \n",
            "Loss: 0.0233547855168581, Accuracy: 0.92 \n",
            "Loss: 0.010887719690799713, Accuracy: 0.97 \n",
            "Loss: 0.013214010745286942, Accuracy: 0.97 \n",
            "Loss: 0.018664108589291573, Accuracy: 0.94 \n",
            "Loss: 0.012819561176002026, Accuracy: 0.95 \n",
            "Loss: 0.007996131666004658, Accuracy: 0.99 \n",
            "Loss: 0.02441019006073475, Accuracy: 0.91 \n",
            "Loss: 0.01779385656118393, Accuracy: 0.95 \n",
            "Loss: 0.011722313240170479, Accuracy: 0.97 \n",
            "Loss: 0.00530336843803525, Accuracy: 0.98 \n",
            "Loss: 0.022085539996623993, Accuracy: 0.96 \n",
            "Loss: 0.018830256536602974, Accuracy: 0.95 \n",
            "Loss: 0.012303085066378117, Accuracy: 0.96 \n",
            "Loss: 0.009790454059839249, Accuracy: 0.98 \n",
            "Loss: 0.010642274282872677, Accuracy: 0.98 \n",
            "Loss: 0.020777812227606773, Accuracy: 0.93 \n",
            "Loss: 0.010050211101770401, Accuracy: 0.96 \n",
            "Loss: 0.009168495424091816, Accuracy: 0.98 \n",
            "Loss: 0.02286485582590103, Accuracy: 0.92 \n",
            "Loss: 0.022133272141218185, Accuracy: 0.96 \n",
            "Loss: 0.01837502419948578, Accuracy: 0.96 \n",
            "Loss: 0.011046702973544598, Accuracy: 0.97 \n",
            "Loss: 0.012694593518972397, Accuracy: 0.97 \n",
            "Loss: 0.008775965310633183, Accuracy: 0.99 \n",
            "Loss: 0.01772439107298851, Accuracy: 0.95 \n",
            "Loss: 0.02117825113236904, Accuracy: 0.93 \n",
            "Loss: 0.007557458244264126, Accuracy: 0.97 \n",
            "Loss: 0.012126668356359005, Accuracy: 0.95 \n",
            "Loss: 0.024342358112335205, Accuracy: 0.92 \n",
            "Loss: 0.022299185395240784, Accuracy: 0.94 \n",
            "Loss: 0.010492328554391861, Accuracy: 0.97 \n",
            "Loss: 0.011691564694046974, Accuracy: 0.98 \n",
            "Loss: 0.01537308655679226, Accuracy: 0.94 \n",
            "Loss: 0.01068834774196148, Accuracy: 0.98 \n",
            "Loss: 0.010104685090482235, Accuracy: 0.97 \n",
            "Loss: 0.010064431466162205, Accuracy: 0.96 \n",
            "Loss: 0.01908990554511547, Accuracy: 0.92 \n",
            "Loss: 0.014897298067808151, Accuracy: 0.95 \n",
            "Loss: 0.029589353129267693, Accuracy: 0.93 \n",
            "Loss: 0.012236894108355045, Accuracy: 0.96 \n",
            "Loss: 0.01787048764526844, Accuracy: 0.95 \n",
            "Loss: 0.016724035143852234, Accuracy: 0.95 \n",
            "Loss: 0.013088399544358253, Accuracy: 0.96 \n",
            "Loss: 0.014303257688879967, Accuracy: 0.95 \n",
            "Loss: 0.018785273656249046, Accuracy: 0.98 \n",
            "Loss: 0.024505984038114548, Accuracy: 0.94 \n",
            "Loss: 0.021988440304994583, Accuracy: 0.92 \n",
            "Loss: 0.011292104609310627, Accuracy: 0.96 \n",
            "Loss: 0.01059393398463726, Accuracy: 0.96 \n",
            "Loss: 0.006840023212134838, Accuracy: 0.98 \n",
            "Loss: 0.010463714599609375, Accuracy: 0.96 \n",
            "Loss: 0.004967367742210627, Accuracy: 1.0 \n",
            "Loss: 0.02834291011095047, Accuracy: 0.93 \n",
            "Loss: 0.016929324716329575, Accuracy: 0.95 \n",
            "Loss: 0.01044496987015009, Accuracy: 0.98 \n",
            "Loss: 0.020590554922819138, Accuracy: 0.94 \n",
            "Loss: 0.009179573506116867, Accuracy: 0.97 \n",
            "Loss: 0.018224967643618584, Accuracy: 0.95 \n",
            "Loss: 0.01259742770344019, Accuracy: 0.95 \n",
            "Loss: 0.016332151368260384, Accuracy: 0.97 \n",
            "Loss: 0.013175925239920616, Accuracy: 0.97 \n",
            "Loss: 0.015964526683092117, Accuracy: 0.96 \n",
            "Loss: 0.019139666110277176, Accuracy: 0.93 \n",
            "Loss: 0.020223965868353844, Accuracy: 0.93 \n",
            "Loss: 0.019092941656708717, Accuracy: 0.94 \n",
            "Loss: 0.010497909039258957, Accuracy: 0.97 \n",
            "Loss: 0.012332985177636147, Accuracy: 0.97 \n",
            "Loss: 0.018841911107301712, Accuracy: 0.95 \n",
            "Loss: 0.013364012353122234, Accuracy: 0.95 \n",
            "Loss: 0.012263065204024315, Accuracy: 0.97 \n",
            "Loss: 0.020138757303357124, Accuracy: 0.97 \n",
            "Loss: 0.01643030345439911, Accuracy: 0.96 \n",
            "Loss: 0.011755160056054592, Accuracy: 0.97 \n",
            "Loss: 0.013045048341155052, Accuracy: 0.96 \n",
            "Loss: 0.03702634200453758, Accuracy: 0.9 \n",
            "Loss: 0.026999710127711296, Accuracy: 0.89 \n",
            "Loss: 0.0113827558234334, Accuracy: 0.96 \n",
            "Loss: 0.018640000373125076, Accuracy: 0.93 \n",
            "Loss: 0.009326241910457611, Accuracy: 0.99 \n",
            "Loss: 0.014596004039049149, Accuracy: 0.95 \n",
            "Loss: 0.013535147532820702, Accuracy: 0.95 \n",
            "Loss: 0.009720328263938427, Accuracy: 0.97 \n",
            "Loss: 0.01529675256460905, Accuracy: 0.96 \n",
            "Loss: 0.013309270143508911, Accuracy: 0.96 \n",
            "Loss: 0.014857522211968899, Accuracy: 0.97 \n",
            "Loss: 0.019410084933042526, Accuracy: 0.94 \n",
            "Loss: 0.018671048805117607, Accuracy: 0.94 \n",
            "Loss: 0.016757583245635033, Accuracy: 0.94 \n",
            "Loss: 0.025407280772924423, Accuracy: 0.95 \n",
            "Loss: 0.007819397374987602, Accuracy: 0.98 \n",
            "Loss: 0.013066397979855537, Accuracy: 0.95 \n",
            "Loss: 0.007167709060013294, Accuracy: 0.99 \n",
            "Loss: 0.007325529586523771, Accuracy: 0.99 \n",
            "Loss: 0.012633906677365303, Accuracy: 0.94 \n",
            "Loss: 0.01343963947147131, Accuracy: 0.96 \n",
            "Loss: 0.03938838467001915, Accuracy: 0.95 \n",
            "Loss: 0.011559157632291317, Accuracy: 0.97 \n",
            "Loss: 0.01700752228498459, Accuracy: 0.97 \n",
            "Loss: 0.018154704943299294, Accuracy: 0.96 \n",
            "Loss: 0.013441730290651321, Accuracy: 0.97 \n",
            "Loss: 0.018686676397919655, Accuracy: 0.93 \n",
            "Loss: 0.01293210033327341, Accuracy: 0.95 \n",
            "Loss: 0.015553084202110767, Accuracy: 0.96 \n",
            "Loss: 0.019282912835478783, Accuracy: 0.96 \n",
            "Loss: 0.013994636945426464, Accuracy: 0.96 \n",
            "Loss: 0.02449614740908146, Accuracy: 0.91 \n",
            "Loss: 0.01717165671288967, Accuracy: 0.96 \n",
            "Loss: 0.01762266457080841, Accuracy: 0.96 \n",
            "Loss: 0.012535392306745052, Accuracy: 0.96 \n",
            "Loss: 0.02811916172504425, Accuracy: 0.92 \n",
            "Loss: 0.00670299306511879, Accuracy: 0.98 \n",
            "Loss: 0.01998821273446083, Accuracy: 0.93 \n",
            "Loss: 0.01563165709376335, Accuracy: 0.93 \n",
            "Loss: 0.019167883321642876, Accuracy: 0.95 \n",
            "Loss: 0.016443101689219475, Accuracy: 0.95 \n",
            "Loss: 0.013172231614589691, Accuracy: 0.97 \n",
            "Loss: 0.018982309848070145, Accuracy: 0.96 \n",
            "Loss: 0.012830939143896103, Accuracy: 0.96 \n",
            "Loss: 0.0177666787058115, Accuracy: 0.96 \n",
            "Loss: 0.01477755792438984, Accuracy: 0.96 \n",
            "Loss: 0.00823841243982315, Accuracy: 0.97 \n",
            "Loss: 0.015393231995403767, Accuracy: 0.95 \n",
            "Loss: 0.014766919426620007, Accuracy: 0.95 \n",
            "Loss: 0.00700526824221015, Accuracy: 0.96 \n",
            "Loss: 0.007036374416202307, Accuracy: 0.99 \n",
            "Loss: 0.013859723694622517, Accuracy: 0.97 \n",
            "Loss: 0.017238112166523933, Accuracy: 0.93 \n",
            "Loss: 0.01032758504152298, Accuracy: 0.98 \n",
            "Loss: 0.008032948710024357, Accuracy: 0.97 \n",
            "Loss: 0.017123183235526085, Accuracy: 0.95 \n",
            "Loss: 0.016726288944482803, Accuracy: 0.94 \n",
            "Loss: 0.01720169000327587, Accuracy: 0.94 \n",
            "Loss: 0.00954992976039648, Accuracy: 0.96 \n",
            "Loss: 0.010817758738994598, Accuracy: 0.97 \n",
            "Loss: 0.017716534435749054, Accuracy: 0.95 \n",
            "Loss: 0.015228565782308578, Accuracy: 0.97 \n",
            "Loss: 0.01284672599285841, Accuracy: 0.97 \n",
            "Loss: 0.010688018053770065, Accuracy: 0.97 \n",
            "Loss: 0.015290813520550728, Accuracy: 0.96 \n",
            "Loss: 0.010928891599178314, Accuracy: 0.97 \n",
            "Loss: 0.012555924244225025, Accuracy: 0.98 \n",
            "Loss: 0.009056898765265942, Accuracy: 0.98 \n",
            "Loss: 0.013929464854300022, Accuracy: 0.97 \n",
            "Loss: 0.012575759552419186, Accuracy: 0.96 \n",
            "Loss: 0.015503100119531155, Accuracy: 0.97 \n",
            "Loss: 0.018374459818005562, Accuracy: 0.97 \n",
            "Loss: 0.009557903744280338, Accuracy: 0.94 \n",
            "Loss: 0.014134344644844532, Accuracy: 0.94 \n",
            "Loss: 0.01705564185976982, Accuracy: 0.94 \n",
            "Loss: 0.029757225885987282, Accuracy: 0.94 \n",
            "Loss: 0.008762191981077194, Accuracy: 0.97 \n",
            "Loss: 0.008044969290494919, Accuracy: 0.97 \n",
            "Loss: 0.013394095934927464, Accuracy: 0.97 \n",
            "Loss: 0.006094369105994701, Accuracy: 0.99 \n",
            "Loss: 0.01641368493437767, Accuracy: 0.95 \n",
            "Loss: 0.011909893713891506, Accuracy: 0.95 \n",
            "Loss: 0.013191897422075272, Accuracy: 0.97 \n",
            "Loss: 0.015657149255275726, Accuracy: 0.96 \n",
            "Loss: 0.01554336678236723, Accuracy: 0.96 \n",
            "Loss: 0.012389177456498146, Accuracy: 0.97 \n",
            "Loss: 0.014365349896252155, Accuracy: 0.96 \n",
            "Loss: 0.006835956126451492, Accuracy: 0.97 \n",
            "Loss: 0.025150300934910774, Accuracy: 0.92 \n",
            "Loss: 0.017024513334035873, Accuracy: 0.95 \n",
            "Loss: 0.008512540720403194, Accuracy: 0.98 \n",
            "Loss: 0.01687425933778286, Accuracy: 0.96 \n",
            "Loss: 0.008771456778049469, Accuracy: 0.96 \n",
            "Loss: 0.02180209942162037, Accuracy: 0.95 \n",
            "Loss: 0.011101157404482365, Accuracy: 0.95 \n",
            "Loss: 0.015876565128564835, Accuracy: 0.94 \n",
            "Loss: 0.020194632932543755, Accuracy: 0.94 \n",
            "Loss: 0.010219024494290352, Accuracy: 0.99 \n",
            "Loss: 0.0196748785674572, Accuracy: 0.93 \n",
            "Loss: 0.0185101255774498, Accuracy: 0.96 \n",
            "Loss: 0.006410109810531139, Accuracy: 1.0 \n",
            "Loss: 0.011495236307382584, Accuracy: 0.96 \n",
            "Loss: 0.019284924492239952, Accuracy: 0.96 \n",
            "Loss: 0.014204391278326511, Accuracy: 0.96 \n",
            "Loss: 0.01064243819564581, Accuracy: 0.97 \n",
            "Loss: 0.01051185093820095, Accuracy: 0.98 \n",
            "Loss: 0.026435229927301407, Accuracy: 0.93 \n",
            "Loss: 0.016323918476700783, Accuracy: 0.93 \n",
            "Loss: 0.013036361895501614, Accuracy: 0.96 \n",
            "Loss: 0.014621669426560402, Accuracy: 0.94 \n",
            "Loss: 0.009333355352282524, Accuracy: 0.97 \n",
            "Loss: 0.010680713690817356, Accuracy: 0.97 \n",
            "Loss: 0.015943342819809914, Accuracy: 0.94 \n",
            "Loss: 0.01670869067311287, Accuracy: 0.97 \n",
            "Loss: 0.0101166358217597, Accuracy: 0.98 \n",
            "Loss: 0.016050584614276886, Accuracy: 0.95 \n",
            "Loss: 0.010341479443013668, Accuracy: 0.96 \n",
            "Loss: 0.007830727845430374, Accuracy: 0.99 \n",
            "Loss: 0.007296090945601463, Accuracy: 0.98 \n",
            "Loss: 0.009488244540989399, Accuracy: 0.97 \n",
            "Loss: 0.015298333950340748, Accuracy: 0.95 \n",
            "Loss: 0.011230736039578915, Accuracy: 0.96 \n",
            "Loss: 0.026892809197306633, Accuracy: 0.95 \n",
            "Loss: 0.009988431818783283, Accuracy: 0.96 \n",
            "Loss: 0.0110178766772151, Accuracy: 0.98 \n",
            "Loss: 0.010226041078567505, Accuracy: 0.96 \n",
            "Loss: 0.009672158397734165, Accuracy: 0.97 \n",
            "Loss: 0.017122561112046242, Accuracy: 0.94 \n",
            "Loss: 0.009594173170626163, Accuracy: 0.97 \n",
            "Loss: 0.024374546483159065, Accuracy: 0.95 \n",
            "Loss: 0.027446594089269638, Accuracy: 0.9 \n",
            "Loss: 0.015706762671470642, Accuracy: 0.94 \n",
            "Loss: 0.008105563931167126, Accuracy: 0.99 \n",
            "Loss: 0.01810488849878311, Accuracy: 0.95 \n",
            "Loss: 0.011678027920424938, Accuracy: 0.95 \n",
            "Loss: 0.015148277394473553, Accuracy: 0.96 \n",
            "Loss: 0.010849987156689167, Accuracy: 0.97 \n",
            "Loss: 0.010202721692621708, Accuracy: 0.98 \n",
            "Loss: 0.006558063440024853, Accuracy: 0.99 \n",
            "Loss: 0.005825560539960861, Accuracy: 0.99 \n",
            "Loss: 0.020276522263884544, Accuracy: 0.95 \n",
            "Loss: 0.009508062154054642, Accuracy: 0.98 \n",
            "Loss: 0.010764310136437416, Accuracy: 0.96 \n",
            "Loss: 0.011354471556842327, Accuracy: 0.98 \n",
            "Loss: 0.010809618048369884, Accuracy: 0.96 \n",
            "Loss: 0.01688675582408905, Accuracy: 0.95 \n",
            "Loss: 0.008843879215419292, Accuracy: 0.96 \n",
            "Loss: 0.005624287761747837, Accuracy: 0.98 \n",
            "Loss: 0.010484784841537476, Accuracy: 0.96 \n",
            "Loss: 0.00801747478544712, Accuracy: 0.97 \n",
            "Loss: 0.01264464296400547, Accuracy: 0.97 \n",
            "Loss: 0.013876207172870636, Accuracy: 0.97 \n",
            "Loss: 0.005652040243148804, Accuracy: 0.97 \n",
            "Loss: 0.008811045438051224, Accuracy: 0.97 \n",
            "Loss: 0.01310009229928255, Accuracy: 0.95 \n",
            "Loss: 0.025184646248817444, Accuracy: 0.96 \n",
            "Loss: 0.011644909158349037, Accuracy: 0.96 \n",
            "Loss: 0.017808223143219948, Accuracy: 0.95 \n",
            "Loss: 0.0074201589450240135, Accuracy: 0.99 \n",
            "Loss: 0.011672534048557281, Accuracy: 0.97 \n",
            "Loss: 0.012848942540585995, Accuracy: 0.96 \n",
            "Loss: 0.008608072064816952, Accuracy: 0.98 \n",
            "Loss: 0.01583121344447136, Accuracy: 0.95 \n",
            "Loss: 0.009022628888487816, Accuracy: 0.99 \n",
            "Loss: 0.007953069172799587, Accuracy: 0.97 \n",
            "Loss: 0.008138621225953102, Accuracy: 0.97 \n",
            "Loss: 0.011476302519440651, Accuracy: 0.96 \n",
            "Loss: 0.008102941326797009, Accuracy: 0.97 \n",
            "Loss: 0.018514981493353844, Accuracy: 0.95 \n",
            "Loss: 0.008462270721793175, Accuracy: 0.98 \n",
            "Loss: 0.007790274452418089, Accuracy: 0.97 \n",
            "Loss: 0.004731917753815651, Accuracy: 0.98 \n",
            "Loss: 0.0197887122631073, Accuracy: 0.95 \n",
            "Loss: 0.012655793689191341, Accuracy: 0.95 \n",
            "Loss: 0.009444823488593102, Accuracy: 0.98 \n",
            "Loss: 0.018537422642111778, Accuracy: 0.94 \n",
            "Loss: 0.016368599608540535, Accuracy: 0.95 \n",
            "Loss: 0.010099491104483604, Accuracy: 0.96 \n",
            "Loss: 0.018871121108531952, Accuracy: 0.94 \n",
            "Loss: 0.011562279425561428, Accuracy: 0.95 \n",
            "Loss: 0.016350798308849335, Accuracy: 0.94 \n",
            "Loss: 0.014510338194668293, Accuracy: 0.94 \n",
            "Loss: 0.005530309863388538, Accuracy: 0.99 \n",
            "Loss: 0.014761477708816528, Accuracy: 0.94 \n",
            "Loss: 0.024079307913780212, Accuracy: 0.92 \n",
            "Loss: 0.01088358648121357, Accuracy: 0.97 \n",
            "Loss: 0.018192101269960403, Accuracy: 0.95 \n",
            "Loss: 0.01439486350864172, Accuracy: 0.95 \n",
            "Loss: 0.01479613408446312, Accuracy: 0.96 \n",
            "Loss: 0.01949654519557953, Accuracy: 0.95 \n",
            "Loss: 0.015292640775442123, Accuracy: 0.95 \n",
            "Loss: 0.012813461013138294, Accuracy: 0.95 \n",
            "Loss: 0.017673693597316742, Accuracy: 0.92 \n",
            "Loss: 0.011024361476302147, Accuracy: 0.97 \n",
            "Loss: 0.007522000465542078, Accuracy: 0.99 \n",
            "Loss: 0.014414995908737183, Accuracy: 0.96 \n",
            "Loss: 0.012468448840081692, Accuracy: 0.95 \n",
            "Loss: 0.013019685633480549, Accuracy: 0.96 \n",
            "Loss: 0.024035992100834846, Accuracy: 0.93 \n",
            "Loss: 0.008681362494826317, Accuracy: 0.96 \n",
            "Loss: 0.02004142850637436, Accuracy: 0.94 \n",
            "Loss: 0.010466170497238636, Accuracy: 0.96 \n",
            "Loss: 0.015402590855956078, Accuracy: 0.95 \n",
            "Loss: 0.01017720252275467, Accuracy: 0.97 \n",
            "Loss: 0.01277429424226284, Accuracy: 0.97 \n",
            "Loss: 0.021251339465379715, Accuracy: 0.94 \n",
            "Loss: 0.017147246748209, Accuracy: 0.93 \n",
            "Loss: 0.0062963212840259075, Accuracy: 0.98 \n",
            "Loss: 0.0185642521828413, Accuracy: 0.94 \n",
            "Loss: 0.013993613421916962, Accuracy: 0.95 \n",
            "Loss: 0.01860016956925392, Accuracy: 0.94 \n",
            "Loss: 0.01814478449523449, Accuracy: 0.95 \n",
            "Loss: 0.009426386095583439, Accuracy: 0.97 \n",
            "Loss: 0.02008967101573944, Accuracy: 0.95 \n",
            "Loss: 0.015140976756811142, Accuracy: 0.97 \n",
            "Loss: 0.007183287292718887, Accuracy: 0.97 \n",
            "Loss: 0.011885464191436768, Accuracy: 0.96 \n",
            "Loss: 0.01434100978076458, Accuracy: 0.94 \n",
            "Loss: 0.014162744395434856, Accuracy: 0.97 \n",
            "Loss: 0.01659989543259144, Accuracy: 0.92 \n",
            "Loss: 0.01936672255396843, Accuracy: 0.95 \n",
            "Loss: 0.019039273262023926, Accuracy: 0.95 \n",
            "Loss: 0.025512268766760826, Accuracy: 0.94 \n",
            "Loss: 0.008169476874172688, Accuracy: 0.98 \n",
            "Loss: 0.01347443088889122, Accuracy: 0.96 \n",
            "Loss: 0.013813209719955921, Accuracy: 0.94 \n",
            "Loss: 0.01182903628796339, Accuracy: 0.96 \n",
            "Loss: 0.010979319922626019, Accuracy: 0.96 \n",
            "Loss: 0.006528887897729874, Accuracy: 0.99 \n",
            "Loss: 0.022177251055836678, Accuracy: 0.94 \n",
            "Loss: 0.013184238225221634, Accuracy: 0.97 \n",
            "Loss: 0.01352771371603012, Accuracy: 0.95 \n",
            "Loss: 0.0062287733890116215, Accuracy: 0.99 \n",
            "Loss: 0.02248484455049038, Accuracy: 0.94 \n",
            "Loss: 0.015963435173034668, Accuracy: 0.96 \n",
            "Loss: 0.023128261789679527, Accuracy: 0.93 \n",
            "Loss: 0.012978533282876015, Accuracy: 0.97 \n",
            "Loss: 0.010593418031930923, Accuracy: 0.96 \n",
            "Loss: 0.01745273545384407, Accuracy: 0.93 \n",
            "Loss: 0.0103686498478055, Accuracy: 0.97 \n",
            "Loss: 0.011250755749642849, Accuracy: 0.98 \n",
            "Loss: 0.013859694823622704, Accuracy: 0.94 \n",
            "Loss: 0.01229054480791092, Accuracy: 0.96 \n",
            "Loss: 0.00784714613109827, Accuracy: 0.99 \n",
            "Loss: 0.020530173555016518, Accuracy: 0.96 \n",
            "Loss: 0.012474311515688896, Accuracy: 0.96 \n",
            "Loss: 0.012241734191775322, Accuracy: 0.96 \n",
            "Loss: 0.021612152457237244, Accuracy: 0.9 \n",
            "Loss: 0.008893292397260666, Accuracy: 0.97 \n",
            "Loss: 0.012884682044386864, Accuracy: 0.98 \n",
            "Loss: 0.02934763953089714, Accuracy: 0.92 \n",
            "Loss: 0.007372383959591389, Accuracy: 0.97 \n",
            "Loss: 0.009126540273427963, Accuracy: 0.97 \n",
            "Loss: 0.015996601432561874, Accuracy: 0.95 \n",
            "Loss: 0.016860658302903175, Accuracy: 0.96 \n",
            "Loss: 0.01317537296563387, Accuracy: 0.98 \n",
            "Loss: 0.013314017094671726, Accuracy: 0.97 \n",
            "Loss: 0.006137697491794825, Accuracy: 0.98 \n",
            "Loss: 0.011375336907804012, Accuracy: 0.97 \n",
            "Loss: 0.013319198973476887, Accuracy: 0.95 \n",
            "Loss: 0.014845875091850758, Accuracy: 0.95 \n",
            "Loss: 0.015674453228712082, Accuracy: 0.96 \n",
            "Loss: 0.010175432078540325, Accuracy: 0.98 \n",
            "Loss: 0.017043836414813995, Accuracy: 0.96 \n",
            "Loss: 0.025681868195533752, Accuracy: 0.95 \n",
            "Loss: 0.012791389599442482, Accuracy: 0.97 \n",
            "Loss: 0.013916193507611752, Accuracy: 0.93 \n",
            "Loss: 0.010894067585468292, Accuracy: 0.97 \n",
            "Loss: 0.005764292553067207, Accuracy: 1.0 \n",
            "Loss: 0.01199304684996605, Accuracy: 0.95 \n",
            "Loss: 0.009258409962058067, Accuracy: 0.98 \n",
            "Loss: 0.014294925145804882, Accuracy: 0.96 \n",
            "Loss: 0.004081724677234888, Accuracy: 0.98 \n",
            "Loss: 0.01994006149470806, Accuracy: 0.93 \n",
            "Loss: 0.005645202938467264, Accuracy: 0.98 \n",
            "Loss: 0.012654310092329979, Accuracy: 0.96 \n",
            "Loss: 0.007407043594866991, Accuracy: 0.98 \n",
            "Loss: 0.01608392223715782, Accuracy: 0.94 \n",
            "Loss: 0.009342593140900135, Accuracy: 0.96 \n",
            "Loss: 0.013916394673287868, Accuracy: 0.95 \n",
            "Loss: 0.010760423727333546, Accuracy: 0.98 \n",
            "Loss: 0.006564912386238575, Accuracy: 0.99 \n",
            "Loss: 0.006051020231097937, Accuracy: 0.99 \n",
            "Loss: 0.011784347705543041, Accuracy: 0.95 \n",
            "Loss: 0.01167207583785057, Accuracy: 0.96 \n",
            "Loss: 0.010219833813607693, Accuracy: 0.96 \n",
            "Loss: 0.014513957314193249, Accuracy: 0.96 \n",
            "Loss: 0.008085351437330246, Accuracy: 0.96 \n",
            "Loss: 0.012811260297894478, Accuracy: 0.96 \n",
            "Loss: 0.007156793959438801, Accuracy: 0.98 \n",
            "Loss: 0.02070847898721695, Accuracy: 0.92 \n",
            "Loss: 0.007487140130251646, Accuracy: 0.98 \n",
            "Loss: 0.031038403511047363, Accuracy: 0.93 \n",
            "Loss: 0.016993844881653786, Accuracy: 0.92 \n",
            "Loss: 0.013339865021407604, Accuracy: 0.97 \n",
            "Loss: 0.006336386315524578, Accuracy: 0.99 \n",
            "Loss: 0.014625308103859425, Accuracy: 0.94 \n",
            "Loss: 0.0060614305548369884, Accuracy: 0.99 \n",
            "Loss: 0.0042183431796729565, Accuracy: 1.0 \n",
            "Loss: 0.005964316893368959, Accuracy: 0.99 \n",
            "Loss: 0.016840077936649323, Accuracy: 0.97 \n",
            "Loss: 0.01107829436659813, Accuracy: 0.96 \n",
            "Loss: 0.020136503502726555, Accuracy: 0.94 \n",
            "Loss: 0.007686906959861517, Accuracy: 0.98 \n",
            "Loss: 0.006997983902692795, Accuracy: 0.99 \n",
            "Loss: 0.015294590033590794, Accuracy: 0.96 \n",
            "Loss: 0.012478438206017017, Accuracy: 0.97 \n",
            "Loss: 0.009868660941720009, Accuracy: 0.95 \n",
            "Loss: 0.00817142054438591, Accuracy: 0.97 \n",
            "Loss: 0.009900607168674469, Accuracy: 0.98 \n",
            "Loss: 0.010852163657546043, Accuracy: 0.95 \n",
            "Loss: 0.008595624007284641, Accuracy: 0.97 \n",
            "Loss: 0.017554717138409615, Accuracy: 0.93 \n",
            "Loss: 0.008011706173419952, Accuracy: 0.97 \n",
            "Loss: 0.005270310211926699, Accuracy: 0.99 \n",
            "Loss: 0.007007553707808256, Accuracy: 0.99 \n",
            "Loss: 0.02185230888426304, Accuracy: 0.94 \n",
            "Loss: 0.014616139233112335, Accuracy: 0.97 \n",
            "Loss: 0.013423326425254345, Accuracy: 0.96 \n",
            "Loss: 0.007564929313957691, Accuracy: 0.98 \n",
            "Loss: 0.010808955878019333, Accuracy: 0.96 \n",
            "Loss: 0.013931728899478912, Accuracy: 0.96 \n",
            "Loss: 0.018619544804096222, Accuracy: 0.94 \n",
            "Loss: 0.013956556096673012, Accuracy: 0.95 \n",
            "Loss: 0.01826365478336811, Accuracy: 0.95 \n",
            "Loss: 0.020328138023614883, Accuracy: 0.93 \n",
            "Loss: 0.015023290179669857, Accuracy: 0.98 \n",
            "Loss: 0.01997394859790802, Accuracy: 0.95 \n",
            "Loss: 0.015587572008371353, Accuracy: 0.97 \n",
            "Loss: 0.010458379983901978, Accuracy: 0.98 \n",
            "Loss: 0.016157079488039017, Accuracy: 0.95 \n",
            "Loss: 0.009679884649813175, Accuracy: 0.97 \n",
            "Loss: 0.022411514073610306, Accuracy: 0.93 \n",
            "Loss: 0.007188895717263222, Accuracy: 0.96 \n",
            "Loss: 0.014514381065964699, Accuracy: 0.96 \n",
            "Loss: 0.010878405533730984, Accuracy: 0.96 \n",
            "Loss: 0.01238283421844244, Accuracy: 0.98 \n",
            "Loss: 0.022402670234441757, Accuracy: 0.96 \n",
            "Loss: 0.012233640998601913, Accuracy: 0.96 \n",
            "Loss: 0.009812130592763424, Accuracy: 0.99 \n",
            "Loss: 0.011656402610242367, Accuracy: 0.97 \n",
            "Loss: 0.012775509618222713, Accuracy: 0.98 \n",
            "Loss: 0.018369732424616814, Accuracy: 0.93 \n",
            "Loss: 0.012942477129399776, Accuracy: 0.96 \n",
            "Loss: 0.00952034443616867, Accuracy: 0.96 \n",
            "Loss: 0.014780254103243351, Accuracy: 0.96 \n",
            "Loss: 0.018139777705073357, Accuracy: 0.94 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLTWvCV79-YE"
      },
      "source": [
        "Wow so this all actually worked.  This is implementing a neural network FROM SCRATCH.  Pretty cool!!  I still have a LONG way to go though, something as \"simple\" as this had me absolutely stumped for so long...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIDpvtWYv4Kh",
        "outputId": "efdb6ffa-6522-4eff-9ad1-1e7db6b96933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(accuracy)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbbcaea1780>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}