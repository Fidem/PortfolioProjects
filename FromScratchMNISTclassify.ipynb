{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FromScratchMNISTclassify.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTLd+PkrEGLTLVb1A5dW5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fidem/PortfolioProjects/blob/main/FromScratchMNISTclassify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkojU8CPTnyF"
      },
      "source": [
        "#This is my attempt at implementing a Neural network from scratch.  I just want to implement a mirror of my tensorflow dense neural network solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nafcXc8TivH"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPXtBNFT19X"
      },
      "source": [
        "Okay I do want to cheat a bit and get the mnist dataset from the tensorflow keras api, its a lot easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiyxt0vtT9R5"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N45NVBtcUFDl",
        "outputId": "c4d402cc-bec0-4c7c-8e93-70d7f7c3d422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihw2tQQVVnVq"
      },
      "source": [
        "Okay so we have our data.  Next is just creating a function for the log softmax activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCIq46OUi01"
      },
      "source": [
        "\n",
        "\n",
        "def logsumexp(x):\n",
        "  a = x.max(axis=1)\n",
        "  return a + np.log(np.exp(x-a.reshape((-1,1))).sum(axis=1))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_H0BCjkAOsF"
      },
      "source": [
        "First we should initialise the network with random weights:\n",
        "\n",
        "So we want to be able to initialise each layer of weights independently, and the weights are randomly sampled from a uniform distribtion between -1 and 1.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on_GYXhT_6Cp"
      },
      "source": [
        "\n",
        "def init_lay(inp, out):\n",
        "  \n",
        "  w = np.random.uniform(-1.,1.,size=(inp,out))/np.sqrt(inp*out)\n",
        "  return w.astype(np.float32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqpAH8hZcNft"
      },
      "source": [
        "NOTE: OLI THE MATHS FOR THIS IS IN YOUR NOTEBOOK, CANT BE BOTHERED WRITING IT ALL IN MARKDOWN/LATEX...\n",
        "\n",
        "Next is the forward propogation step.  For this, we need to activate the neurons (using weighted sum of inputs like linear regression), then run this through a non-linear activation function, then we just forward propogate to the output layer\n",
        "\n",
        "So first we calc the dot product between our x input and the weights in layer 1.\n",
        "\n",
        "Then we use the relu activation function.  Aka - if value is less than 0, =0, otherwise value = value.  This can be done easily using np.maximum, comparing x_l1, and an array of 0s.\n",
        "\n",
        "\n",
        "NOTE: This is the holy grail code chunk, that has both the forward propogation (Passing input through to the output layer), and the backward propogation (Calculating derivatives of activation functions and layers to get the errors for gradient descent step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhKv4gHpeXbW"
      },
      "source": [
        "def forward_pass(x, y):\n",
        "  out = np.zeros((len(y),10),np.float32)\n",
        "  out[range(out.shape[0]),y] = 1\n",
        "\n",
        "  x_l1 = x.dot(layer1)\n",
        "  x_relu = np.maximum(x_l1,0) \n",
        "  x_l2 = x_relu.dot(layer2)\n",
        "  x_logsum = x_l2 - logsumexp(x_l2).reshape((-1,1))\n",
        "  loss = (-out * x_logsum).mean(axis=1)\n",
        "  d_out = -out / len(y)\n",
        "\n",
        "  return d_out, x_logsum, x_relu, loss, x_l2\n",
        "\n",
        "def backprop(x,y,dout,xlsm,xrelu):\n",
        "  dx_lsm = dout - np.exp(xlsm) * dout.sum(axis=1).reshape((-1,1))\n",
        "  d_l2 = xrelu.T.dot(dx_lsm)\n",
        "  dx_relu = dx_lsm.dot(layer2.T)\n",
        "  dx_l1 = (xrelu > 0).astype(np.float32) * dx_relu\n",
        "  d_l1 = x.T.dot(dx_l1)\n",
        "  return d_l1, d_l2\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjm5cTUVqiHf"
      },
      "source": [
        "Okay that was awful and tragic and weird, but hopefully it will work?  Just need to actually write the loops and include gradient descent on the weights using the d_l1 and d_l2.\n",
        "\n",
        "Need to set the global learning rate variable and initialise the layers.\n",
        "\n",
        "Also idk, with each epoch i dont really really want to train with all 60,000 images.  thats kinda stoopid.  Why not just get a sample of lets say 100 images to train with each epoch??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpKQTh9Fqh3k",
        "outputId": "0b986c53-bfae-4032-e432-55b5cb583475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "learnrate = 0.001\n",
        "np.random.seed(1)\n",
        "layer1 = init_lay(784,128)\n",
        "layer2 = init_lay(128,10)\n",
        "losses = []\n",
        "accuracy = []\n",
        "\n",
        "for i in range(1000):\n",
        "  rand = np.random.randint(0,x_train.shape[0],size=100)\n",
        "  X = x_train[rand].reshape(-1,28*28)\n",
        "  Y = y_train[rand]\n",
        "  \n",
        "  dout, xlsm, xrelu, loss, x_l2 = forward_pass(X,Y)\n",
        "  losses.append(loss.mean())\n",
        "  dl1, dl2 = backprop(X,Y,dout,xlsm,xrelu)\n",
        "\n",
        "  res = np.argmax(x_l2,axis=1)\n",
        "  acc = (res==Y).mean()\n",
        "  accuracy.append(acc)\n",
        "\n",
        "  layer1 -= learnrate*dl1\n",
        "  layer2 -= learnrate*dl2\n",
        "  print(\"Loss: {}, Accuracy: {} \".format(loss.mean(),acc))\n",
        "  \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.23726403713226318, Accuracy: 0.08 \n",
            "Loss: 0.16998174786567688, Accuracy: 0.48 \n",
            "Loss: 0.18517711758613586, Accuracy: 0.3 \n",
            "Loss: 0.19661197066307068, Accuracy: 0.48 \n",
            "Loss: 0.19114786386489868, Accuracy: 0.42 \n",
            "Loss: 0.14040516316890717, Accuracy: 0.58 \n",
            "Loss: 0.1199321374297142, Accuracy: 0.64 \n",
            "Loss: 0.0970398336648941, Accuracy: 0.75 \n",
            "Loss: 0.11021822690963745, Accuracy: 0.59 \n",
            "Loss: 0.09148547053337097, Accuracy: 0.74 \n",
            "Loss: 0.07898381352424622, Accuracy: 0.75 \n",
            "Loss: 0.09106595069169998, Accuracy: 0.72 \n",
            "Loss: 0.09995058178901672, Accuracy: 0.7 \n",
            "Loss: 0.06995347142219543, Accuracy: 0.76 \n",
            "Loss: 0.07262308150529861, Accuracy: 0.76 \n",
            "Loss: 0.09345162659883499, Accuracy: 0.69 \n",
            "Loss: 0.07439443469047546, Accuracy: 0.66 \n",
            "Loss: 0.1173638254404068, Accuracy: 0.59 \n",
            "Loss: 0.09036669135093689, Accuracy: 0.71 \n",
            "Loss: 0.08407264947891235, Accuracy: 0.68 \n",
            "Loss: 0.06224149838089943, Accuracy: 0.79 \n",
            "Loss: 0.05475805327296257, Accuracy: 0.8 \n",
            "Loss: 0.06959982216358185, Accuracy: 0.78 \n",
            "Loss: 0.05892091244459152, Accuracy: 0.82 \n",
            "Loss: 0.04686989262700081, Accuracy: 0.87 \n",
            "Loss: 0.0561915822327137, Accuracy: 0.84 \n",
            "Loss: 0.06372564285993576, Accuracy: 0.8 \n",
            "Loss: 0.050063274800777435, Accuracy: 0.82 \n",
            "Loss: 0.0513734370470047, Accuracy: 0.83 \n",
            "Loss: 0.04249810799956322, Accuracy: 0.89 \n",
            "Loss: 0.0306907519698143, Accuracy: 0.92 \n",
            "Loss: 0.04591277241706848, Accuracy: 0.82 \n",
            "Loss: 0.052624378353357315, Accuracy: 0.85 \n",
            "Loss: 0.0647500678896904, Accuracy: 0.82 \n",
            "Loss: 0.041765302419662476, Accuracy: 0.86 \n",
            "Loss: 0.05704697221517563, Accuracy: 0.81 \n",
            "Loss: 0.04670428857207298, Accuracy: 0.88 \n",
            "Loss: 0.03263634070754051, Accuracy: 0.88 \n",
            "Loss: 0.03969226032495499, Accuracy: 0.9 \n",
            "Loss: 0.035708941519260406, Accuracy: 0.89 \n",
            "Loss: 0.05992840230464935, Accuracy: 0.82 \n",
            "Loss: 0.05251401290297508, Accuracy: 0.82 \n",
            "Loss: 0.035360079258680344, Accuracy: 0.92 \n",
            "Loss: 0.05761121213436127, Accuracy: 0.81 \n",
            "Loss: 0.05472612753510475, Accuracy: 0.81 \n",
            "Loss: 0.05212705582380295, Accuracy: 0.86 \n",
            "Loss: 0.053434401750564575, Accuracy: 0.84 \n",
            "Loss: 0.04174701124429703, Accuracy: 0.88 \n",
            "Loss: 0.05489417910575867, Accuracy: 0.83 \n",
            "Loss: 0.050896625965833664, Accuracy: 0.81 \n",
            "Loss: 0.03088737651705742, Accuracy: 0.94 \n",
            "Loss: 0.04177379980683327, Accuracy: 0.86 \n",
            "Loss: 0.042792025953531265, Accuracy: 0.87 \n",
            "Loss: 0.04444339871406555, Accuracy: 0.85 \n",
            "Loss: 0.040092144161462784, Accuracy: 0.91 \n",
            "Loss: 0.042391978204250336, Accuracy: 0.88 \n",
            "Loss: 0.02423846162855625, Accuracy: 0.94 \n",
            "Loss: 0.024465860798954964, Accuracy: 0.94 \n",
            "Loss: 0.023828618228435516, Accuracy: 0.97 \n",
            "Loss: 0.028797823935747147, Accuracy: 0.94 \n",
            "Loss: 0.020032918080687523, Accuracy: 0.97 \n",
            "Loss: 0.03121272474527359, Accuracy: 0.92 \n",
            "Loss: 0.04550137370824814, Accuracy: 0.87 \n",
            "Loss: 0.021260254085063934, Accuracy: 0.94 \n",
            "Loss: 0.037700388580560684, Accuracy: 0.89 \n",
            "Loss: 0.03034062869846821, Accuracy: 0.9 \n",
            "Loss: 0.03697032481431961, Accuracy: 0.88 \n",
            "Loss: 0.026558689773082733, Accuracy: 0.91 \n",
            "Loss: 0.028423743322491646, Accuracy: 0.89 \n",
            "Loss: 0.03293723240494728, Accuracy: 0.91 \n",
            "Loss: 0.03806744143366814, Accuracy: 0.87 \n",
            "Loss: 0.04187062382698059, Accuracy: 0.87 \n",
            "Loss: 0.022073695436120033, Accuracy: 0.93 \n",
            "Loss: 0.04132992774248123, Accuracy: 0.9 \n",
            "Loss: 0.03485959395766258, Accuracy: 0.89 \n",
            "Loss: 0.030887141823768616, Accuracy: 0.92 \n",
            "Loss: 0.02523585967719555, Accuracy: 0.93 \n",
            "Loss: 0.04185860604047775, Accuracy: 0.88 \n",
            "Loss: 0.018971659243106842, Accuracy: 0.94 \n",
            "Loss: 0.039636027067899704, Accuracy: 0.88 \n",
            "Loss: 0.03763056918978691, Accuracy: 0.91 \n",
            "Loss: 0.030105022713541985, Accuracy: 0.91 \n",
            "Loss: 0.03221392259001732, Accuracy: 0.91 \n",
            "Loss: 0.039285603910684586, Accuracy: 0.91 \n",
            "Loss: 0.03245515376329422, Accuracy: 0.91 \n",
            "Loss: 0.026930253952741623, Accuracy: 0.93 \n",
            "Loss: 0.021875932812690735, Accuracy: 0.95 \n",
            "Loss: 0.02229735441505909, Accuracy: 0.94 \n",
            "Loss: 0.02359548769891262, Accuracy: 0.95 \n",
            "Loss: 0.0270504429936409, Accuracy: 0.93 \n",
            "Loss: 0.03838684782385826, Accuracy: 0.9 \n",
            "Loss: 0.04085741564631462, Accuracy: 0.93 \n",
            "Loss: 0.034500185400247574, Accuracy: 0.91 \n",
            "Loss: 0.039381396025419235, Accuracy: 0.87 \n",
            "Loss: 0.03957666456699371, Accuracy: 0.91 \n",
            "Loss: 0.017768913879990578, Accuracy: 0.96 \n",
            "Loss: 0.01924167573451996, Accuracy: 0.94 \n",
            "Loss: 0.021835384890437126, Accuracy: 0.96 \n",
            "Loss: 0.042978014796972275, Accuracy: 0.87 \n",
            "Loss: 0.03357500210404396, Accuracy: 0.9 \n",
            "Loss: 0.024119609966874123, Accuracy: 0.94 \n",
            "Loss: 0.015029501169919968, Accuracy: 0.96 \n",
            "Loss: 0.03393954411149025, Accuracy: 0.91 \n",
            "Loss: 0.03847397863864899, Accuracy: 0.88 \n",
            "Loss: 0.028417663648724556, Accuracy: 0.91 \n",
            "Loss: 0.03769825026392937, Accuracy: 0.86 \n",
            "Loss: 0.030921030789613724, Accuracy: 0.93 \n",
            "Loss: 0.02608785778284073, Accuracy: 0.92 \n",
            "Loss: 0.032777588814496994, Accuracy: 0.87 \n",
            "Loss: 0.02554481476545334, Accuracy: 0.92 \n",
            "Loss: 0.02464982680976391, Accuracy: 0.93 \n",
            "Loss: 0.0363575778901577, Accuracy: 0.92 \n",
            "Loss: 0.025704137980937958, Accuracy: 0.93 \n",
            "Loss: 0.031298473477363586, Accuracy: 0.94 \n",
            "Loss: 0.046941112726926804, Accuracy: 0.9 \n",
            "Loss: 0.017350859940052032, Accuracy: 0.95 \n",
            "Loss: 0.02260550670325756, Accuracy: 0.92 \n",
            "Loss: 0.03069067746400833, Accuracy: 0.9 \n",
            "Loss: 0.030535902827978134, Accuracy: 0.86 \n",
            "Loss: 0.03429068997502327, Accuracy: 0.88 \n",
            "Loss: 0.03294225037097931, Accuracy: 0.86 \n",
            "Loss: 0.024983737617731094, Accuracy: 0.93 \n",
            "Loss: 0.016133159399032593, Accuracy: 0.96 \n",
            "Loss: 0.025113439187407494, Accuracy: 0.92 \n",
            "Loss: 0.027004104107618332, Accuracy: 0.91 \n",
            "Loss: 0.016949806362390518, Accuracy: 0.99 \n",
            "Loss: 0.03666596859693527, Accuracy: 0.88 \n",
            "Loss: 0.042823996394872665, Accuracy: 0.87 \n",
            "Loss: 0.04032623767852783, Accuracy: 0.92 \n",
            "Loss: 0.020608356222510338, Accuracy: 0.94 \n",
            "Loss: 0.03322829306125641, Accuracy: 0.92 \n",
            "Loss: 0.03340732678771019, Accuracy: 0.92 \n",
            "Loss: 0.021073665469884872, Accuracy: 0.95 \n",
            "Loss: 0.020347822457551956, Accuracy: 0.96 \n",
            "Loss: 0.027675125747919083, Accuracy: 0.88 \n",
            "Loss: 0.025112424045801163, Accuracy: 0.93 \n",
            "Loss: 0.025734510272741318, Accuracy: 0.93 \n",
            "Loss: 0.02251041866838932, Accuracy: 0.93 \n",
            "Loss: 0.030630620196461678, Accuracy: 0.94 \n",
            "Loss: 0.02263517677783966, Accuracy: 0.92 \n",
            "Loss: 0.022394394502043724, Accuracy: 0.94 \n",
            "Loss: 0.04602888971567154, Accuracy: 0.86 \n",
            "Loss: 0.05077921226620674, Accuracy: 0.86 \n",
            "Loss: 0.021978309378027916, Accuracy: 0.93 \n",
            "Loss: 0.026357686147093773, Accuracy: 0.93 \n",
            "Loss: 0.026150420308113098, Accuracy: 0.92 \n",
            "Loss: 0.016149304807186127, Accuracy: 0.96 \n",
            "Loss: 0.03839290887117386, Accuracy: 0.9 \n",
            "Loss: 0.021526053547859192, Accuracy: 0.92 \n",
            "Loss: 0.019345935434103012, Accuracy: 0.95 \n",
            "Loss: 0.03551985323429108, Accuracy: 0.92 \n",
            "Loss: 0.02288118191063404, Accuracy: 0.93 \n",
            "Loss: 0.025738291442394257, Accuracy: 0.91 \n",
            "Loss: 0.021708013489842415, Accuracy: 0.95 \n",
            "Loss: 0.03892921656370163, Accuracy: 0.89 \n",
            "Loss: 0.03691709786653519, Accuracy: 0.87 \n",
            "Loss: 0.0318949818611145, Accuracy: 0.92 \n",
            "Loss: 0.026752082630991936, Accuracy: 0.9 \n",
            "Loss: 0.03484124317765236, Accuracy: 0.86 \n",
            "Loss: 0.014798461459577084, Accuracy: 0.96 \n",
            "Loss: 0.019023597240447998, Accuracy: 0.94 \n",
            "Loss: 0.026515500620007515, Accuracy: 0.9 \n",
            "Loss: 0.020319120958447456, Accuracy: 0.93 \n",
            "Loss: 0.024088973179459572, Accuracy: 0.95 \n",
            "Loss: 0.012817034497857094, Accuracy: 0.98 \n",
            "Loss: 0.030954895541071892, Accuracy: 0.93 \n",
            "Loss: 0.024376556277275085, Accuracy: 0.93 \n",
            "Loss: 0.013250724412500858, Accuracy: 0.96 \n",
            "Loss: 0.02873256616294384, Accuracy: 0.89 \n",
            "Loss: 0.023166919127106667, Accuracy: 0.92 \n",
            "Loss: 0.03793320804834366, Accuracy: 0.91 \n",
            "Loss: 0.01864391379058361, Accuracy: 0.97 \n",
            "Loss: 0.029455695301294327, Accuracy: 0.92 \n",
            "Loss: 0.0357116162776947, Accuracy: 0.86 \n",
            "Loss: 0.015805838629603386, Accuracy: 0.95 \n",
            "Loss: 0.025106992572546005, Accuracy: 0.92 \n",
            "Loss: 0.019119111821055412, Accuracy: 0.97 \n",
            "Loss: 0.010440411977469921, Accuracy: 0.98 \n",
            "Loss: 0.02929702214896679, Accuracy: 0.89 \n",
            "Loss: 0.03706217557191849, Accuracy: 0.9 \n",
            "Loss: 0.019107094034552574, Accuracy: 0.95 \n",
            "Loss: 0.027068672701716423, Accuracy: 0.9 \n",
            "Loss: 0.027360206469893456, Accuracy: 0.91 \n",
            "Loss: 0.014968977309763432, Accuracy: 0.96 \n",
            "Loss: 0.014648362062871456, Accuracy: 0.96 \n",
            "Loss: 0.0361102819442749, Accuracy: 0.91 \n",
            "Loss: 0.02516535110771656, Accuracy: 0.94 \n",
            "Loss: 0.02056787721812725, Accuracy: 0.94 \n",
            "Loss: 0.03377550467848778, Accuracy: 0.93 \n",
            "Loss: 0.027114098891615868, Accuracy: 0.93 \n",
            "Loss: 0.02058366686105728, Accuracy: 0.94 \n",
            "Loss: 0.02574915438890457, Accuracy: 0.93 \n",
            "Loss: 0.024283811450004578, Accuracy: 0.94 \n",
            "Loss: 0.031939513981342316, Accuracy: 0.89 \n",
            "Loss: 0.023537522181868553, Accuracy: 0.94 \n",
            "Loss: 0.017099356278777122, Accuracy: 0.94 \n",
            "Loss: 0.024761183187365532, Accuracy: 0.94 \n",
            "Loss: 0.022786300629377365, Accuracy: 0.92 \n",
            "Loss: 0.028626592829823494, Accuracy: 0.92 \n",
            "Loss: 0.019953930750489235, Accuracy: 0.92 \n",
            "Loss: 0.02341756597161293, Accuracy: 0.94 \n",
            "Loss: 0.019649563357234, Accuracy: 0.94 \n",
            "Loss: 0.026053516194224358, Accuracy: 0.92 \n",
            "Loss: 0.02547670342028141, Accuracy: 0.93 \n",
            "Loss: 0.021575255319476128, Accuracy: 0.96 \n",
            "Loss: 0.021586427465081215, Accuracy: 0.94 \n",
            "Loss: 0.022463059052824974, Accuracy: 0.94 \n",
            "Loss: 0.017686892300844193, Accuracy: 0.97 \n",
            "Loss: 0.028994617983698845, Accuracy: 0.9 \n",
            "Loss: 0.023161184042692184, Accuracy: 0.92 \n",
            "Loss: 0.016108183190226555, Accuracy: 0.95 \n",
            "Loss: 0.02169499360024929, Accuracy: 0.95 \n",
            "Loss: 0.025085140019655228, Accuracy: 0.93 \n",
            "Loss: 0.02429501712322235, Accuracy: 0.94 \n",
            "Loss: 0.01721244491636753, Accuracy: 0.96 \n",
            "Loss: 0.030545182526111603, Accuracy: 0.9 \n",
            "Loss: 0.022035544738173485, Accuracy: 0.91 \n",
            "Loss: 0.021266067400574684, Accuracy: 0.96 \n",
            "Loss: 0.01502477191388607, Accuracy: 0.96 \n",
            "Loss: 0.03388917073607445, Accuracy: 0.9 \n",
            "Loss: 0.015557128004729748, Accuracy: 0.96 \n",
            "Loss: 0.018917512148618698, Accuracy: 0.93 \n",
            "Loss: 0.024312054738402367, Accuracy: 0.93 \n",
            "Loss: 0.024766335263848305, Accuracy: 0.93 \n",
            "Loss: 0.021036483347415924, Accuracy: 0.95 \n",
            "Loss: 0.024469273164868355, Accuracy: 0.95 \n",
            "Loss: 0.019140997901558876, Accuracy: 0.96 \n",
            "Loss: 0.0160609632730484, Accuracy: 0.96 \n",
            "Loss: 0.03135453537106514, Accuracy: 0.93 \n",
            "Loss: 0.019919121637940407, Accuracy: 0.94 \n",
            "Loss: 0.01613631471991539, Accuracy: 0.95 \n",
            "Loss: 0.015576343983411789, Accuracy: 0.95 \n",
            "Loss: 0.020944491028785706, Accuracy: 0.95 \n",
            "Loss: 0.02463061548769474, Accuracy: 0.94 \n",
            "Loss: 0.024100983515381813, Accuracy: 0.93 \n",
            "Loss: 0.02610681764781475, Accuracy: 0.93 \n",
            "Loss: 0.014459491707384586, Accuracy: 0.95 \n",
            "Loss: 0.024014564231038094, Accuracy: 0.93 \n",
            "Loss: 0.025850849226117134, Accuracy: 0.95 \n",
            "Loss: 0.013712812215089798, Accuracy: 0.94 \n",
            "Loss: 0.01723802648484707, Accuracy: 0.97 \n",
            "Loss: 0.017424656078219414, Accuracy: 0.95 \n",
            "Loss: 0.019777951762080193, Accuracy: 0.96 \n",
            "Loss: 0.022641001269221306, Accuracy: 0.95 \n",
            "Loss: 0.024267330765724182, Accuracy: 0.92 \n",
            "Loss: 0.012850724160671234, Accuracy: 0.98 \n",
            "Loss: 0.03476651757955551, Accuracy: 0.89 \n",
            "Loss: 0.017545007169246674, Accuracy: 0.95 \n",
            "Loss: 0.018822044134140015, Accuracy: 0.93 \n",
            "Loss: 0.023548949509859085, Accuracy: 0.93 \n",
            "Loss: 0.023913485929369926, Accuracy: 0.93 \n",
            "Loss: 0.021674174815416336, Accuracy: 0.95 \n",
            "Loss: 0.018316442146897316, Accuracy: 0.97 \n",
            "Loss: 0.01142815500497818, Accuracy: 0.95 \n",
            "Loss: 0.012489023618400097, Accuracy: 0.97 \n",
            "Loss: 0.01731775514781475, Accuracy: 0.95 \n",
            "Loss: 0.017637863755226135, Accuracy: 0.94 \n",
            "Loss: 0.02522231638431549, Accuracy: 0.93 \n",
            "Loss: 0.02294114977121353, Accuracy: 0.9 \n",
            "Loss: 0.026734953746199608, Accuracy: 0.94 \n",
            "Loss: 0.021784406155347824, Accuracy: 0.94 \n",
            "Loss: 0.02655654028058052, Accuracy: 0.93 \n",
            "Loss: 0.028032558038830757, Accuracy: 0.88 \n",
            "Loss: 0.014982539229094982, Accuracy: 0.97 \n",
            "Loss: 0.02642698772251606, Accuracy: 0.89 \n",
            "Loss: 0.022554168477654457, Accuracy: 0.93 \n",
            "Loss: 0.01372026838362217, Accuracy: 0.95 \n",
            "Loss: 0.010572541505098343, Accuracy: 0.98 \n",
            "Loss: 0.018997186794877052, Accuracy: 0.95 \n",
            "Loss: 0.01468505896627903, Accuracy: 0.97 \n",
            "Loss: 0.019280236214399338, Accuracy: 0.92 \n",
            "Loss: 0.028098691254854202, Accuracy: 0.91 \n",
            "Loss: 0.02443063072860241, Accuracy: 0.93 \n",
            "Loss: 0.01814710721373558, Accuracy: 0.94 \n",
            "Loss: 0.00788580160588026, Accuracy: 0.99 \n",
            "Loss: 0.008951960131525993, Accuracy: 0.99 \n",
            "Loss: 0.01302875392138958, Accuracy: 0.98 \n",
            "Loss: 0.020369382575154305, Accuracy: 0.92 \n",
            "Loss: 0.023922188207507133, Accuracy: 0.94 \n",
            "Loss: 0.01267109252512455, Accuracy: 0.97 \n",
            "Loss: 0.013851514086127281, Accuracy: 0.97 \n",
            "Loss: 0.022271623834967613, Accuracy: 0.94 \n",
            "Loss: 0.023867616429924965, Accuracy: 0.94 \n",
            "Loss: 0.011091466061770916, Accuracy: 0.98 \n",
            "Loss: 0.024299385026097298, Accuracy: 0.94 \n",
            "Loss: 0.02147284708917141, Accuracy: 0.93 \n",
            "Loss: 0.01679837517440319, Accuracy: 0.96 \n",
            "Loss: 0.026188557967543602, Accuracy: 0.94 \n",
            "Loss: 0.015627216547727585, Accuracy: 0.95 \n",
            "Loss: 0.028326943516731262, Accuracy: 0.91 \n",
            "Loss: 0.028072252869606018, Accuracy: 0.93 \n",
            "Loss: 0.01207596343010664, Accuracy: 0.95 \n",
            "Loss: 0.011151195503771305, Accuracy: 0.97 \n",
            "Loss: 0.013722075149416924, Accuracy: 0.98 \n",
            "Loss: 0.018111251294612885, Accuracy: 0.97 \n",
            "Loss: 0.01524127833545208, Accuracy: 0.94 \n",
            "Loss: 0.022218933328986168, Accuracy: 0.92 \n",
            "Loss: 0.023900603875517845, Accuracy: 0.92 \n",
            "Loss: 0.02334561198949814, Accuracy: 0.95 \n",
            "Loss: 0.01884651556611061, Accuracy: 0.95 \n",
            "Loss: 0.019698843359947205, Accuracy: 0.96 \n",
            "Loss: 0.013597885146737099, Accuracy: 0.97 \n",
            "Loss: 0.02652192860841751, Accuracy: 0.92 \n",
            "Loss: 0.01659747026860714, Accuracy: 0.94 \n",
            "Loss: 0.024613071233034134, Accuracy: 0.9 \n",
            "Loss: 0.014333002269268036, Accuracy: 0.96 \n",
            "Loss: 0.022638242691755295, Accuracy: 0.95 \n",
            "Loss: 0.019336208701133728, Accuracy: 0.93 \n",
            "Loss: 0.016477853059768677, Accuracy: 0.94 \n",
            "Loss: 0.011802950873970985, Accuracy: 0.95 \n",
            "Loss: 0.01735595241189003, Accuracy: 0.94 \n",
            "Loss: 0.03493542969226837, Accuracy: 0.92 \n",
            "Loss: 0.02298547327518463, Accuracy: 0.92 \n",
            "Loss: 0.015236841514706612, Accuracy: 0.97 \n",
            "Loss: 0.017108643427491188, Accuracy: 0.95 \n",
            "Loss: 0.014460613951086998, Accuracy: 0.95 \n",
            "Loss: 0.014962390996515751, Accuracy: 0.95 \n",
            "Loss: 0.02692013792693615, Accuracy: 0.95 \n",
            "Loss: 0.01892479509115219, Accuracy: 0.93 \n",
            "Loss: 0.019061889499425888, Accuracy: 0.95 \n",
            "Loss: 0.03227357566356659, Accuracy: 0.9 \n",
            "Loss: 0.03481653705239296, Accuracy: 0.91 \n",
            "Loss: 0.015281599946320057, Accuracy: 0.92 \n",
            "Loss: 0.015651578083634377, Accuracy: 0.95 \n",
            "Loss: 0.019591987133026123, Accuracy: 0.95 \n",
            "Loss: 0.03485511243343353, Accuracy: 0.91 \n",
            "Loss: 0.009888552129268646, Accuracy: 0.96 \n",
            "Loss: 0.01988304778933525, Accuracy: 0.92 \n",
            "Loss: 0.015527759678661823, Accuracy: 0.95 \n",
            "Loss: 0.01735677197575569, Accuracy: 0.94 \n",
            "Loss: 0.01775985211133957, Accuracy: 0.95 \n",
            "Loss: 0.040216293185949326, Accuracy: 0.92 \n",
            "Loss: 0.024100804701447487, Accuracy: 0.93 \n",
            "Loss: 0.034979093819856644, Accuracy: 0.91 \n",
            "Loss: 0.037363190203905106, Accuracy: 0.92 \n",
            "Loss: 0.04021841660141945, Accuracy: 0.89 \n",
            "Loss: 0.027720332145690918, Accuracy: 0.95 \n",
            "Loss: 0.017123693600296974, Accuracy: 0.94 \n",
            "Loss: 0.016170185059309006, Accuracy: 0.94 \n",
            "Loss: 0.023586701601743698, Accuracy: 0.94 \n",
            "Loss: 0.020283401012420654, Accuracy: 0.94 \n",
            "Loss: 0.01901034638285637, Accuracy: 0.96 \n",
            "Loss: 0.02624637633562088, Accuracy: 0.91 \n",
            "Loss: 0.011143796145915985, Accuracy: 0.96 \n",
            "Loss: 0.019656412303447723, Accuracy: 0.95 \n",
            "Loss: 0.02135307528078556, Accuracy: 0.94 \n",
            "Loss: 0.018356841057538986, Accuracy: 0.94 \n",
            "Loss: 0.014827459119260311, Accuracy: 0.96 \n",
            "Loss: 0.019620642066001892, Accuracy: 0.96 \n",
            "Loss: 0.012671248987317085, Accuracy: 0.97 \n",
            "Loss: 0.025365378707647324, Accuracy: 0.95 \n",
            "Loss: 0.026367798447608948, Accuracy: 0.94 \n",
            "Loss: 0.025955768302083015, Accuracy: 0.92 \n",
            "Loss: 0.021286195144057274, Accuracy: 0.93 \n",
            "Loss: 0.023231854662299156, Accuracy: 0.92 \n",
            "Loss: 0.013881117105484009, Accuracy: 0.96 \n",
            "Loss: 0.014364118687808514, Accuracy: 0.98 \n",
            "Loss: 0.011865931563079357, Accuracy: 0.98 \n",
            "Loss: 0.014522671699523926, Accuracy: 0.95 \n",
            "Loss: 0.01684631034731865, Accuracy: 0.93 \n",
            "Loss: 0.021558726206421852, Accuracy: 0.94 \n",
            "Loss: 0.012770133092999458, Accuracy: 0.96 \n",
            "Loss: 0.02066553756594658, Accuracy: 0.93 \n",
            "Loss: 0.020600872114300728, Accuracy: 0.94 \n",
            "Loss: 0.022875791415572166, Accuracy: 0.94 \n",
            "Loss: 0.01408110186457634, Accuracy: 0.97 \n",
            "Loss: 0.018528681248426437, Accuracy: 0.94 \n",
            "Loss: 0.03786085173487663, Accuracy: 0.89 \n",
            "Loss: 0.018541861325502396, Accuracy: 0.95 \n",
            "Loss: 0.014736281707882881, Accuracy: 0.95 \n",
            "Loss: 0.009400895796716213, Accuracy: 0.99 \n",
            "Loss: 0.013729692436754704, Accuracy: 0.98 \n",
            "Loss: 0.018172727897763252, Accuracy: 0.95 \n",
            "Loss: 0.011916576884686947, Accuracy: 0.98 \n",
            "Loss: 0.013215302489697933, Accuracy: 0.97 \n",
            "Loss: 0.027369583025574684, Accuracy: 0.95 \n",
            "Loss: 0.014643436297774315, Accuracy: 0.96 \n",
            "Loss: 0.024529753252863884, Accuracy: 0.93 \n",
            "Loss: 0.019083339720964432, Accuracy: 0.93 \n",
            "Loss: 0.017420358955860138, Accuracy: 0.95 \n",
            "Loss: 0.014108961448073387, Accuracy: 0.95 \n",
            "Loss: 0.016633428633213043, Accuracy: 0.95 \n",
            "Loss: 0.027713075280189514, Accuracy: 0.92 \n",
            "Loss: 0.021100446581840515, Accuracy: 0.91 \n",
            "Loss: 0.01552611868828535, Accuracy: 0.95 \n",
            "Loss: 0.0187121219933033, Accuracy: 0.96 \n",
            "Loss: 0.01519987266510725, Accuracy: 0.94 \n",
            "Loss: 0.012182124890387058, Accuracy: 0.94 \n",
            "Loss: 0.015971189364790916, Accuracy: 0.96 \n",
            "Loss: 0.02408965304493904, Accuracy: 0.92 \n",
            "Loss: 0.026311293244361877, Accuracy: 0.93 \n",
            "Loss: 0.011943782679736614, Accuracy: 0.97 \n",
            "Loss: 0.021073170006275177, Accuracy: 0.95 \n",
            "Loss: 0.01793503947556019, Accuracy: 0.95 \n",
            "Loss: 0.010336472652852535, Accuracy: 0.97 \n",
            "Loss: 0.02300892397761345, Accuracy: 0.93 \n",
            "Loss: 0.01889413595199585, Accuracy: 0.93 \n",
            "Loss: 0.030305953696370125, Accuracy: 0.92 \n",
            "Loss: 0.01629010960459709, Accuracy: 0.95 \n",
            "Loss: 0.02785244956612587, Accuracy: 0.95 \n",
            "Loss: 0.018220651894807816, Accuracy: 0.94 \n",
            "Loss: 0.015955759212374687, Accuracy: 0.97 \n",
            "Loss: 0.020362555980682373, Accuracy: 0.96 \n",
            "Loss: 0.03539266064763069, Accuracy: 0.92 \n",
            "Loss: 0.02038571797311306, Accuracy: 0.92 \n",
            "Loss: 0.009779440239071846, Accuracy: 0.96 \n",
            "Loss: 0.018817376345396042, Accuracy: 0.92 \n",
            "Loss: 0.021357323974370956, Accuracy: 0.96 \n",
            "Loss: 0.018415112048387527, Accuracy: 0.94 \n",
            "Loss: 0.021706175059080124, Accuracy: 0.92 \n",
            "Loss: 0.01937215030193329, Accuracy: 0.95 \n",
            "Loss: 0.02759353071451187, Accuracy: 0.93 \n",
            "Loss: 0.020236313343048096, Accuracy: 0.92 \n",
            "Loss: 0.015176265500485897, Accuracy: 0.96 \n",
            "Loss: 0.013573652133345604, Accuracy: 0.97 \n",
            "Loss: 0.02802993729710579, Accuracy: 0.92 \n",
            "Loss: 0.013076203875243664, Accuracy: 0.96 \n",
            "Loss: 0.01701546274125576, Accuracy: 0.93 \n",
            "Loss: 0.010111698880791664, Accuracy: 0.99 \n",
            "Loss: 0.015500776469707489, Accuracy: 0.97 \n",
            "Loss: 0.02146283909678459, Accuracy: 0.93 \n",
            "Loss: 0.017509689554572105, Accuracy: 0.95 \n",
            "Loss: 0.02079147845506668, Accuracy: 0.91 \n",
            "Loss: 0.01957731507718563, Accuracy: 0.93 \n",
            "Loss: 0.017622310668230057, Accuracy: 0.94 \n",
            "Loss: 0.01754515990614891, Accuracy: 0.94 \n",
            "Loss: 0.017543990164995193, Accuracy: 0.97 \n",
            "Loss: 0.014461435377597809, Accuracy: 0.95 \n",
            "Loss: 0.015914492309093475, Accuracy: 0.96 \n",
            "Loss: 0.007222948130220175, Accuracy: 0.99 \n",
            "Loss: 0.011091990396380424, Accuracy: 0.98 \n",
            "Loss: 0.019404342398047447, Accuracy: 0.96 \n",
            "Loss: 0.01404457725584507, Accuracy: 0.96 \n",
            "Loss: 0.014508824795484543, Accuracy: 0.96 \n",
            "Loss: 0.013140627183020115, Accuracy: 0.97 \n",
            "Loss: 0.01660403423011303, Accuracy: 0.96 \n",
            "Loss: 0.01586194336414337, Accuracy: 0.96 \n",
            "Loss: 0.009699608199298382, Accuracy: 0.97 \n",
            "Loss: 0.01022692583501339, Accuracy: 0.99 \n",
            "Loss: 0.011763411574065685, Accuracy: 0.99 \n",
            "Loss: 0.019084274768829346, Accuracy: 0.96 \n",
            "Loss: 0.01637343317270279, Accuracy: 0.95 \n",
            "Loss: 0.021170280873775482, Accuracy: 0.91 \n",
            "Loss: 0.0285459253937006, Accuracy: 0.92 \n",
            "Loss: 0.017008904367685318, Accuracy: 0.94 \n",
            "Loss: 0.013331819325685501, Accuracy: 0.93 \n",
            "Loss: 0.00794752687215805, Accuracy: 0.98 \n",
            "Loss: 0.012074045836925507, Accuracy: 0.97 \n",
            "Loss: 0.012708890251815319, Accuracy: 0.96 \n",
            "Loss: 0.009557307697832584, Accuracy: 0.96 \n",
            "Loss: 0.00690038688480854, Accuracy: 0.99 \n",
            "Loss: 0.029261786490678787, Accuracy: 0.91 \n",
            "Loss: 0.019873248413205147, Accuracy: 0.95 \n",
            "Loss: 0.0322590097784996, Accuracy: 0.89 \n",
            "Loss: 0.017549555748701096, Accuracy: 0.94 \n",
            "Loss: 0.02853374183177948, Accuracy: 0.95 \n",
            "Loss: 0.020782820880413055, Accuracy: 0.97 \n",
            "Loss: 0.02208981104195118, Accuracy: 0.91 \n",
            "Loss: 0.00886525772511959, Accuracy: 0.99 \n",
            "Loss: 0.009682640433311462, Accuracy: 0.96 \n",
            "Loss: 0.017652466893196106, Accuracy: 0.96 \n",
            "Loss: 0.011972682550549507, Accuracy: 0.95 \n",
            "Loss: 0.01174005027860403, Accuracy: 0.97 \n",
            "Loss: 0.015288935974240303, Accuracy: 0.95 \n",
            "Loss: 0.011287779547274113, Accuracy: 0.97 \n",
            "Loss: 0.012677079066634178, Accuracy: 0.97 \n",
            "Loss: 0.009478047490119934, Accuracy: 0.99 \n",
            "Loss: 0.019706539809703827, Accuracy: 0.93 \n",
            "Loss: 0.017901288345456123, Accuracy: 0.97 \n",
            "Loss: 0.010521701537072659, Accuracy: 0.98 \n",
            "Loss: 0.024092072620987892, Accuracy: 0.92 \n",
            "Loss: 0.01384933665394783, Accuracy: 0.97 \n",
            "Loss: 0.018380099907517433, Accuracy: 0.96 \n",
            "Loss: 0.020735181868076324, Accuracy: 0.94 \n",
            "Loss: 0.01859561912715435, Accuracy: 0.95 \n",
            "Loss: 0.01875152438879013, Accuracy: 0.96 \n",
            "Loss: 0.017393283545970917, Accuracy: 0.97 \n",
            "Loss: 0.009902658872306347, Accuracy: 0.98 \n",
            "Loss: 0.010892966762185097, Accuracy: 0.95 \n",
            "Loss: 0.011143402196466923, Accuracy: 0.96 \n",
            "Loss: 0.022929653525352478, Accuracy: 0.94 \n",
            "Loss: 0.02555658295750618, Accuracy: 0.91 \n",
            "Loss: 0.02573636919260025, Accuracy: 0.92 \n",
            "Loss: 0.008729086257517338, Accuracy: 0.96 \n",
            "Loss: 0.025459112599492073, Accuracy: 0.95 \n",
            "Loss: 0.015362589620053768, Accuracy: 0.95 \n",
            "Loss: 0.01674521155655384, Accuracy: 0.94 \n",
            "Loss: 0.010265814140439034, Accuracy: 0.96 \n",
            "Loss: 0.018353622406721115, Accuracy: 0.94 \n",
            "Loss: 0.020702378824353218, Accuracy: 0.93 \n",
            "Loss: 0.014178411103785038, Accuracy: 0.95 \n",
            "Loss: 0.010918362997472286, Accuracy: 0.96 \n",
            "Loss: 0.0158417746424675, Accuracy: 0.93 \n",
            "Loss: 0.0070709423162043095, Accuracy: 0.99 \n",
            "Loss: 0.018313027918338776, Accuracy: 0.94 \n",
            "Loss: 0.01693091355264187, Accuracy: 0.93 \n",
            "Loss: 0.01365023571997881, Accuracy: 0.95 \n",
            "Loss: 0.020452184602618217, Accuracy: 0.93 \n",
            "Loss: 0.01873347908258438, Accuracy: 0.93 \n",
            "Loss: 0.022959984838962555, Accuracy: 0.92 \n",
            "Loss: 0.02421513944864273, Accuracy: 0.94 \n",
            "Loss: 0.01805037446320057, Accuracy: 0.94 \n",
            "Loss: 0.013320092111825943, Accuracy: 0.95 \n",
            "Loss: 0.009139821864664555, Accuracy: 0.98 \n",
            "Loss: 0.016099847853183746, Accuracy: 0.94 \n",
            "Loss: 0.01358105055987835, Accuracy: 0.96 \n",
            "Loss: 0.01821010373532772, Accuracy: 0.95 \n",
            "Loss: 0.017395267263054848, Accuracy: 0.95 \n",
            "Loss: 0.026949908584356308, Accuracy: 0.92 \n",
            "Loss: 0.013103166595101357, Accuracy: 0.96 \n",
            "Loss: 0.01599010080099106, Accuracy: 0.94 \n",
            "Loss: 0.016891799867153168, Accuracy: 0.93 \n",
            "Loss: 0.018661201000213623, Accuracy: 0.95 \n",
            "Loss: 0.012243445962667465, Accuracy: 0.96 \n",
            "Loss: 0.01732678897678852, Accuracy: 0.94 \n",
            "Loss: 0.0089576356112957, Accuracy: 0.98 \n",
            "Loss: 0.02075294405221939, Accuracy: 0.94 \n",
            "Loss: 0.011723794974386692, Accuracy: 0.97 \n",
            "Loss: 0.022645045071840286, Accuracy: 0.94 \n",
            "Loss: 0.008664393797516823, Accuracy: 0.98 \n",
            "Loss: 0.014500818215310574, Accuracy: 0.96 \n",
            "Loss: 0.02113405615091324, Accuracy: 0.93 \n",
            "Loss: 0.01671735756099224, Accuracy: 0.95 \n",
            "Loss: 0.019523583352565765, Accuracy: 0.92 \n",
            "Loss: 0.018003182485699654, Accuracy: 0.96 \n",
            "Loss: 0.018981946632266045, Accuracy: 0.93 \n",
            "Loss: 0.020526692271232605, Accuracy: 0.95 \n",
            "Loss: 0.022199800238013268, Accuracy: 0.91 \n",
            "Loss: 0.01655762270092964, Accuracy: 0.95 \n",
            "Loss: 0.0060235667042434216, Accuracy: 0.98 \n",
            "Loss: 0.015015286393463612, Accuracy: 0.97 \n",
            "Loss: 0.009004106745123863, Accuracy: 0.98 \n",
            "Loss: 0.010405574925243855, Accuracy: 0.98 \n",
            "Loss: 0.01573096588253975, Accuracy: 0.96 \n",
            "Loss: 0.014906884171068668, Accuracy: 0.94 \n",
            "Loss: 0.025263624265789986, Accuracy: 0.96 \n",
            "Loss: 0.012866148725152016, Accuracy: 0.98 \n",
            "Loss: 0.009152370505034924, Accuracy: 0.97 \n",
            "Loss: 0.018070707097649574, Accuracy: 0.95 \n",
            "Loss: 0.01022337470203638, Accuracy: 0.99 \n",
            "Loss: 0.006548138801008463, Accuracy: 1.0 \n",
            "Loss: 0.011057441122829914, Accuracy: 0.97 \n",
            "Loss: 0.020739223808050156, Accuracy: 0.93 \n",
            "Loss: 0.006905407179147005, Accuracy: 0.98 \n",
            "Loss: 0.020216189324855804, Accuracy: 0.95 \n",
            "Loss: 0.01563739776611328, Accuracy: 0.96 \n",
            "Loss: 0.008692266419529915, Accuracy: 0.98 \n",
            "Loss: 0.019117435440421104, Accuracy: 0.94 \n",
            "Loss: 0.020851921290159225, Accuracy: 0.95 \n",
            "Loss: 0.015210572630167007, Accuracy: 0.96 \n",
            "Loss: 0.014023209922015667, Accuracy: 0.95 \n",
            "Loss: 0.024540510028600693, Accuracy: 0.92 \n",
            "Loss: 0.01108958013355732, Accuracy: 0.97 \n",
            "Loss: 0.01677660271525383, Accuracy: 0.93 \n",
            "Loss: 0.014977333135902882, Accuracy: 0.98 \n",
            "Loss: 0.01224977895617485, Accuracy: 0.97 \n",
            "Loss: 0.03698379173874855, Accuracy: 0.91 \n",
            "Loss: 0.015144134871661663, Accuracy: 0.96 \n",
            "Loss: 0.012728426605463028, Accuracy: 0.97 \n",
            "Loss: 0.02504231408238411, Accuracy: 0.91 \n",
            "Loss: 0.02537543512880802, Accuracy: 0.94 \n",
            "Loss: 0.009983809664845467, Accuracy: 0.96 \n",
            "Loss: 0.024172503501176834, Accuracy: 0.92 \n",
            "Loss: 0.025705961510539055, Accuracy: 0.9 \n",
            "Loss: 0.017478398978710175, Accuracy: 0.95 \n",
            "Loss: 0.01696023717522621, Accuracy: 0.95 \n",
            "Loss: 0.007628023624420166, Accuracy: 0.99 \n",
            "Loss: 0.015455538406968117, Accuracy: 0.96 \n",
            "Loss: 0.01684379205107689, Accuracy: 0.95 \n",
            "Loss: 0.01345107052475214, Accuracy: 0.92 \n",
            "Loss: 0.011486695148050785, Accuracy: 0.98 \n",
            "Loss: 0.01733163744211197, Accuracy: 0.96 \n",
            "Loss: 0.015357316471636295, Accuracy: 0.95 \n",
            "Loss: 0.01675109751522541, Accuracy: 0.96 \n",
            "Loss: 0.009900251403450966, Accuracy: 0.98 \n",
            "Loss: 0.019354574382305145, Accuracy: 0.95 \n",
            "Loss: 0.02448849007487297, Accuracy: 0.93 \n",
            "Loss: 0.028551023453474045, Accuracy: 0.93 \n",
            "Loss: 0.014347296208143234, Accuracy: 0.96 \n",
            "Loss: 0.015798021107912064, Accuracy: 0.96 \n",
            "Loss: 0.02768130972981453, Accuracy: 0.93 \n",
            "Loss: 0.014182922430336475, Accuracy: 0.96 \n",
            "Loss: 0.016285108402371407, Accuracy: 0.96 \n",
            "Loss: 0.014094611629843712, Accuracy: 0.96 \n",
            "Loss: 0.013599615544080734, Accuracy: 0.97 \n",
            "Loss: 0.010425028391182423, Accuracy: 0.97 \n",
            "Loss: 0.03335542976856232, Accuracy: 0.92 \n",
            "Loss: 0.01467491127550602, Accuracy: 0.96 \n",
            "Loss: 0.02138763666152954, Accuracy: 0.93 \n",
            "Loss: 0.016086013987660408, Accuracy: 0.96 \n",
            "Loss: 0.011584384366869926, Accuracy: 0.96 \n",
            "Loss: 0.00981839932501316, Accuracy: 0.97 \n",
            "Loss: 0.018515892326831818, Accuracy: 0.95 \n",
            "Loss: 0.019149966537952423, Accuracy: 0.97 \n",
            "Loss: 0.03121829591691494, Accuracy: 0.86 \n",
            "Loss: 0.011229162104427814, Accuracy: 0.96 \n",
            "Loss: 0.019773047417402267, Accuracy: 0.94 \n",
            "Loss: 0.013142174109816551, Accuracy: 0.97 \n",
            "Loss: 0.01808795891702175, Accuracy: 0.93 \n",
            "Loss: 0.012591980397701263, Accuracy: 0.98 \n",
            "Loss: 0.01124551985412836, Accuracy: 0.96 \n",
            "Loss: 0.012762285768985748, Accuracy: 0.97 \n",
            "Loss: 0.009997132234275341, Accuracy: 0.98 \n",
            "Loss: 0.011729569174349308, Accuracy: 0.97 \n",
            "Loss: 0.023407604545354843, Accuracy: 0.95 \n",
            "Loss: 0.015930742025375366, Accuracy: 0.95 \n",
            "Loss: 0.01738239824771881, Accuracy: 0.94 \n",
            "Loss: 0.01605892740190029, Accuracy: 0.94 \n",
            "Loss: 0.012597091495990753, Accuracy: 0.95 \n",
            "Loss: 0.021769138053059578, Accuracy: 0.94 \n",
            "Loss: 0.01085344236344099, Accuracy: 0.95 \n",
            "Loss: 0.016478806734085083, Accuracy: 0.97 \n",
            "Loss: 0.023284511640667915, Accuracy: 0.92 \n",
            "Loss: 0.011155095882713795, Accuracy: 0.94 \n",
            "Loss: 0.016227174550294876, Accuracy: 0.94 \n",
            "Loss: 0.019231557846069336, Accuracy: 0.95 \n",
            "Loss: 0.014583957381546497, Accuracy: 0.97 \n",
            "Loss: 0.01409237366169691, Accuracy: 0.95 \n",
            "Loss: 0.006820918992161751, Accuracy: 0.99 \n",
            "Loss: 0.009121913462877274, Accuracy: 0.97 \n",
            "Loss: 0.008334296755492687, Accuracy: 0.98 \n",
            "Loss: 0.009563235566020012, Accuracy: 0.97 \n",
            "Loss: 0.009771930985152721, Accuracy: 0.99 \n",
            "Loss: 0.02052011340856552, Accuracy: 0.91 \n",
            "Loss: 0.008962156251072884, Accuracy: 0.98 \n",
            "Loss: 0.018402237445116043, Accuracy: 0.95 \n",
            "Loss: 0.007952200248837471, Accuracy: 0.97 \n",
            "Loss: 0.009500974789261818, Accuracy: 0.98 \n",
            "Loss: 0.027112998068332672, Accuracy: 0.95 \n",
            "Loss: 0.017250562086701393, Accuracy: 0.95 \n",
            "Loss: 0.01528339833021164, Accuracy: 0.96 \n",
            "Loss: 0.014478377066552639, Accuracy: 0.96 \n",
            "Loss: 0.008132897317409515, Accuracy: 0.97 \n",
            "Loss: 0.013253247365355492, Accuracy: 0.98 \n",
            "Loss: 0.008795268833637238, Accuracy: 0.97 \n",
            "Loss: 0.010865823365747929, Accuracy: 0.98 \n",
            "Loss: 0.012077603489160538, Accuracy: 0.94 \n",
            "Loss: 0.01655447669327259, Accuracy: 0.95 \n",
            "Loss: 0.0278619471937418, Accuracy: 0.95 \n",
            "Loss: 0.014354276470839977, Accuracy: 0.93 \n",
            "Loss: 0.007935830391943455, Accuracy: 0.98 \n",
            "Loss: 0.011594129726290703, Accuracy: 0.96 \n",
            "Loss: 0.007102761883288622, Accuracy: 1.0 \n",
            "Loss: 0.02256528101861477, Accuracy: 0.94 \n",
            "Loss: 0.031682487577199936, Accuracy: 0.94 \n",
            "Loss: 0.008126771077513695, Accuracy: 0.98 \n",
            "Loss: 0.026862338185310364, Accuracy: 0.93 \n",
            "Loss: 0.009869584813714027, Accuracy: 0.97 \n",
            "Loss: 0.01186313759535551, Accuracy: 0.97 \n",
            "Loss: 0.023033471778035164, Accuracy: 0.93 \n",
            "Loss: 0.016086265444755554, Accuracy: 0.95 \n",
            "Loss: 0.018062537536025047, Accuracy: 0.96 \n",
            "Loss: 0.030156057327985764, Accuracy: 0.92 \n",
            "Loss: 0.03379729017615318, Accuracy: 0.9 \n",
            "Loss: 0.020912477746605873, Accuracy: 0.96 \n",
            "Loss: 0.01211182214319706, Accuracy: 0.97 \n",
            "Loss: 0.010959126055240631, Accuracy: 0.98 \n",
            "Loss: 0.008137294091284275, Accuracy: 0.99 \n",
            "Loss: 0.024077706038951874, Accuracy: 0.92 \n",
            "Loss: 0.004248857032507658, Accuracy: 1.0 \n",
            "Loss: 0.012883796356618404, Accuracy: 0.96 \n",
            "Loss: 0.008399509824812412, Accuracy: 0.96 \n",
            "Loss: 0.009739367291331291, Accuracy: 0.98 \n",
            "Loss: 0.009133519604802132, Accuracy: 0.97 \n",
            "Loss: 0.013375746086239815, Accuracy: 0.95 \n",
            "Loss: 0.01796983927488327, Accuracy: 0.96 \n",
            "Loss: 0.020772192627191544, Accuracy: 0.95 \n",
            "Loss: 0.014111961238086224, Accuracy: 0.95 \n",
            "Loss: 0.02270645834505558, Accuracy: 0.93 \n",
            "Loss: 0.01890089549124241, Accuracy: 0.97 \n",
            "Loss: 0.01962728425860405, Accuracy: 0.93 \n",
            "Loss: 0.01894931122660637, Accuracy: 0.95 \n",
            "Loss: 0.010115081444382668, Accuracy: 0.97 \n",
            "Loss: 0.019786356016993523, Accuracy: 0.92 \n",
            "Loss: 0.00697398791089654, Accuracy: 0.99 \n",
            "Loss: 0.022613465785980225, Accuracy: 0.93 \n",
            "Loss: 0.013638753443956375, Accuracy: 0.96 \n",
            "Loss: 0.021239615976810455, Accuracy: 0.93 \n",
            "Loss: 0.012263258919119835, Accuracy: 0.99 \n",
            "Loss: 0.008385858498513699, Accuracy: 0.98 \n",
            "Loss: 0.019243977963924408, Accuracy: 0.93 \n",
            "Loss: 0.019341638311743736, Accuracy: 0.97 \n",
            "Loss: 0.006180777680128813, Accuracy: 0.98 \n",
            "Loss: 0.009810792282223701, Accuracy: 0.97 \n",
            "Loss: 0.03175552934408188, Accuracy: 0.92 \n",
            "Loss: 0.027027681469917297, Accuracy: 0.96 \n",
            "Loss: 0.014573990367352962, Accuracy: 0.96 \n",
            "Loss: 0.012416212819516659, Accuracy: 0.97 \n",
            "Loss: 0.016935961320996284, Accuracy: 0.95 \n",
            "Loss: 0.01489649061113596, Accuracy: 0.96 \n",
            "Loss: 0.01508843433111906, Accuracy: 0.94 \n",
            "Loss: 0.011166377924382687, Accuracy: 0.96 \n",
            "Loss: 0.011807935312390327, Accuracy: 0.96 \n",
            "Loss: 0.014193160459399223, Accuracy: 0.94 \n",
            "Loss: 0.014201254583895206, Accuracy: 0.94 \n",
            "Loss: 0.01370399259030819, Accuracy: 0.95 \n",
            "Loss: 0.026707982644438744, Accuracy: 0.92 \n",
            "Loss: 0.009377613663673401, Accuracy: 0.97 \n",
            "Loss: 0.013184833340346813, Accuracy: 0.97 \n",
            "Loss: 0.013038809411227703, Accuracy: 0.96 \n",
            "Loss: 0.03451133146882057, Accuracy: 0.91 \n",
            "Loss: 0.014821355231106281, Accuracy: 0.93 \n",
            "Loss: 0.016150502488017082, Accuracy: 0.94 \n",
            "Loss: 0.010478794574737549, Accuracy: 0.98 \n",
            "Loss: 0.01004104595631361, Accuracy: 0.98 \n",
            "Loss: 0.009283853694796562, Accuracy: 0.96 \n",
            "Loss: 0.005961569957435131, Accuracy: 0.99 \n",
            "Loss: 0.013604354113340378, Accuracy: 0.97 \n",
            "Loss: 0.006946784444153309, Accuracy: 0.98 \n",
            "Loss: 0.01501992717385292, Accuracy: 0.94 \n",
            "Loss: 0.01089369785040617, Accuracy: 0.98 \n",
            "Loss: 0.014489267021417618, Accuracy: 0.97 \n",
            "Loss: 0.024134723469614983, Accuracy: 0.95 \n",
            "Loss: 0.012100376188755035, Accuracy: 0.96 \n",
            "Loss: 0.01916934922337532, Accuracy: 0.97 \n",
            "Loss: 0.008763986639678478, Accuracy: 0.97 \n",
            "Loss: 0.010684042237699032, Accuracy: 0.97 \n",
            "Loss: 0.010229190811514854, Accuracy: 0.95 \n",
            "Loss: 0.017040090635418892, Accuracy: 0.96 \n",
            "Loss: 0.015616758726537228, Accuracy: 0.96 \n",
            "Loss: 0.00859508290886879, Accuracy: 0.97 \n",
            "Loss: 0.015222218818962574, Accuracy: 0.98 \n",
            "Loss: 0.009852264076471329, Accuracy: 0.95 \n",
            "Loss: 0.0184270478785038, Accuracy: 0.94 \n",
            "Loss: 0.032592106610536575, Accuracy: 0.88 \n",
            "Loss: 0.014268836006522179, Accuracy: 0.97 \n",
            "Loss: 0.013536845333874226, Accuracy: 0.96 \n",
            "Loss: 0.012893538922071457, Accuracy: 0.96 \n",
            "Loss: 0.017873521894216537, Accuracy: 0.95 \n",
            "Loss: 0.018961135298013687, Accuracy: 0.94 \n",
            "Loss: 0.012280872091650963, Accuracy: 0.98 \n",
            "Loss: 0.013653856702148914, Accuracy: 0.95 \n",
            "Loss: 0.012478720396757126, Accuracy: 0.98 \n",
            "Loss: 0.011775421909987926, Accuracy: 0.96 \n",
            "Loss: 0.011128957383334637, Accuracy: 0.95 \n",
            "Loss: 0.014156961813569069, Accuracy: 0.97 \n",
            "Loss: 0.012352656573057175, Accuracy: 0.97 \n",
            "Loss: 0.013255152851343155, Accuracy: 0.96 \n",
            "Loss: 0.017979448661208153, Accuracy: 0.95 \n",
            "Loss: 0.009043971076607704, Accuracy: 0.97 \n",
            "Loss: 0.009057862684130669, Accuracy: 0.97 \n",
            "Loss: 0.014125061221420765, Accuracy: 0.95 \n",
            "Loss: 0.01725088059902191, Accuracy: 0.95 \n",
            "Loss: 0.014003323391079903, Accuracy: 0.95 \n",
            "Loss: 0.007742652669548988, Accuracy: 0.98 \n",
            "Loss: 0.005595070775598288, Accuracy: 1.0 \n",
            "Loss: 0.023710180073976517, Accuracy: 0.93 \n",
            "Loss: 0.02008652128279209, Accuracy: 0.96 \n",
            "Loss: 0.006349116563796997, Accuracy: 0.98 \n",
            "Loss: 0.018704690039157867, Accuracy: 0.94 \n",
            "Loss: 0.015308314934372902, Accuracy: 0.96 \n",
            "Loss: 0.015770288184285164, Accuracy: 0.95 \n",
            "Loss: 0.012809926643967628, Accuracy: 0.98 \n",
            "Loss: 0.012400645762681961, Accuracy: 0.96 \n",
            "Loss: 0.008294295519590378, Accuracy: 0.98 \n",
            "Loss: 0.014649591408669949, Accuracy: 0.96 \n",
            "Loss: 0.013005144894123077, Accuracy: 0.94 \n",
            "Loss: 0.016707798466086388, Accuracy: 0.96 \n",
            "Loss: 0.013966181315481663, Accuracy: 0.96 \n",
            "Loss: 0.011427286081016064, Accuracy: 0.95 \n",
            "Loss: 0.007841072976589203, Accuracy: 0.98 \n",
            "Loss: 0.018269089981913567, Accuracy: 0.96 \n",
            "Loss: 0.034043021500110626, Accuracy: 0.88 \n",
            "Loss: 0.009107951074838638, Accuracy: 0.96 \n",
            "Loss: 0.010832656174898148, Accuracy: 0.96 \n",
            "Loss: 0.019457075744867325, Accuracy: 0.95 \n",
            "Loss: 0.012442036531865597, Accuracy: 0.97 \n",
            "Loss: 0.005667475517839193, Accuracy: 0.98 \n",
            "Loss: 0.010275149717926979, Accuracy: 0.99 \n",
            "Loss: 0.0095145795494318, Accuracy: 0.98 \n",
            "Loss: 0.004570540972054005, Accuracy: 1.0 \n",
            "Loss: 0.005918374750763178, Accuracy: 0.98 \n",
            "Loss: 0.015866868197917938, Accuracy: 0.95 \n",
            "Loss: 0.015349716879427433, Accuracy: 0.95 \n",
            "Loss: 0.006596541963517666, Accuracy: 0.98 \n",
            "Loss: 0.013665893115103245, Accuracy: 0.96 \n",
            "Loss: 0.014392564073204994, Accuracy: 0.95 \n",
            "Loss: 0.021202998235821724, Accuracy: 0.93 \n",
            "Loss: 0.022556759417057037, Accuracy: 0.9 \n",
            "Loss: 0.006184735335409641, Accuracy: 0.98 \n",
            "Loss: 0.007955820299685001, Accuracy: 0.99 \n",
            "Loss: 0.017563223838806152, Accuracy: 0.96 \n",
            "Loss: 0.011691032908856869, Accuracy: 0.96 \n",
            "Loss: 0.018808985128998756, Accuracy: 0.96 \n",
            "Loss: 0.00637170672416687, Accuracy: 1.0 \n",
            "Loss: 0.016700653359293938, Accuracy: 0.96 \n",
            "Loss: 0.010185645893216133, Accuracy: 0.97 \n",
            "Loss: 0.013514062389731407, Accuracy: 0.98 \n",
            "Loss: 0.010993564501404762, Accuracy: 0.98 \n",
            "Loss: 0.02367321401834488, Accuracy: 0.97 \n",
            "Loss: 0.013052876107394695, Accuracy: 0.94 \n",
            "Loss: 0.013886114582419395, Accuracy: 0.95 \n",
            "Loss: 0.010379436425864697, Accuracy: 0.99 \n",
            "Loss: 0.006398595403879881, Accuracy: 0.99 \n",
            "Loss: 0.014170673675835133, Accuracy: 0.95 \n",
            "Loss: 0.013590889051556587, Accuracy: 0.95 \n",
            "Loss: 0.010288380086421967, Accuracy: 0.98 \n",
            "Loss: 0.018419677391648293, Accuracy: 0.97 \n",
            "Loss: 0.014005408622324467, Accuracy: 0.97 \n",
            "Loss: 0.010499182157218456, Accuracy: 0.97 \n",
            "Loss: 0.00844116136431694, Accuracy: 0.99 \n",
            "Loss: 0.019550830125808716, Accuracy: 0.95 \n",
            "Loss: 0.008634572848677635, Accuracy: 0.97 \n",
            "Loss: 0.013392406515777111, Accuracy: 0.96 \n",
            "Loss: 0.0115860840305686, Accuracy: 0.95 \n",
            "Loss: 0.010451487265527248, Accuracy: 0.95 \n",
            "Loss: 0.021563997492194176, Accuracy: 0.94 \n",
            "Loss: 0.011892917566001415, Accuracy: 0.97 \n",
            "Loss: 0.017781490460038185, Accuracy: 0.95 \n",
            "Loss: 0.013908040709793568, Accuracy: 0.96 \n",
            "Loss: 0.018106481060385704, Accuracy: 0.95 \n",
            "Loss: 0.014376260340213776, Accuracy: 0.96 \n",
            "Loss: 0.011797801591455936, Accuracy: 0.96 \n",
            "Loss: 0.014058922417461872, Accuracy: 0.95 \n",
            "Loss: 0.01486576534807682, Accuracy: 0.97 \n",
            "Loss: 0.007621018681675196, Accuracy: 0.99 \n",
            "Loss: 0.010095014236867428, Accuracy: 0.97 \n",
            "Loss: 0.009364303201436996, Accuracy: 0.98 \n",
            "Loss: 0.0191135723143816, Accuracy: 0.96 \n",
            "Loss: 0.015228810720145702, Accuracy: 0.96 \n",
            "Loss: 0.011709519661962986, Accuracy: 0.97 \n",
            "Loss: 0.005015457049012184, Accuracy: 0.99 \n",
            "Loss: 0.014935090206563473, Accuracy: 0.93 \n",
            "Loss: 0.011487760581076145, Accuracy: 0.96 \n",
            "Loss: 0.006227448116987944, Accuracy: 0.99 \n",
            "Loss: 0.02289811335504055, Accuracy: 0.93 \n",
            "Loss: 0.006916357204318047, Accuracy: 0.99 \n",
            "Loss: 0.008531452156603336, Accuracy: 0.97 \n",
            "Loss: 0.017262015491724014, Accuracy: 0.96 \n",
            "Loss: 0.0073436410166323185, Accuracy: 0.98 \n",
            "Loss: 0.015287811867892742, Accuracy: 0.95 \n",
            "Loss: 0.02326899580657482, Accuracy: 0.93 \n",
            "Loss: 0.018064068630337715, Accuracy: 0.94 \n",
            "Loss: 0.018458791077136993, Accuracy: 0.95 \n",
            "Loss: 0.011702507734298706, Accuracy: 0.96 \n",
            "Loss: 0.013764999806880951, Accuracy: 0.95 \n",
            "Loss: 0.02335887961089611, Accuracy: 0.93 \n",
            "Loss: 0.021106576547026634, Accuracy: 0.92 \n",
            "Loss: 0.012470829300582409, Accuracy: 0.96 \n",
            "Loss: 0.015604978427290916, Accuracy: 0.95 \n",
            "Loss: 0.011598723940551281, Accuracy: 0.97 \n",
            "Loss: 0.009245322085916996, Accuracy: 0.98 \n",
            "Loss: 0.01647108606994152, Accuracy: 0.97 \n",
            "Loss: 0.014807317405939102, Accuracy: 0.94 \n",
            "Loss: 0.012033726088702679, Accuracy: 0.97 \n",
            "Loss: 0.010603138245642185, Accuracy: 0.99 \n",
            "Loss: 0.025479674339294434, Accuracy: 0.94 \n",
            "Loss: 0.008388949558138847, Accuracy: 0.99 \n",
            "Loss: 0.014741403982043266, Accuracy: 0.96 \n",
            "Loss: 0.017321234568953514, Accuracy: 0.91 \n",
            "Loss: 0.01498437486588955, Accuracy: 0.95 \n",
            "Loss: 0.013093626126646996, Accuracy: 0.98 \n",
            "Loss: 0.0333917997777462, Accuracy: 0.92 \n",
            "Loss: 0.012207841500639915, Accuracy: 0.97 \n",
            "Loss: 0.009852133691310883, Accuracy: 0.96 \n",
            "Loss: 0.01800907589495182, Accuracy: 0.93 \n",
            "Loss: 0.017952410504221916, Accuracy: 0.93 \n",
            "Loss: 0.01262818556278944, Accuracy: 0.95 \n",
            "Loss: 0.012748033739626408, Accuracy: 0.96 \n",
            "Loss: 0.016611693426966667, Accuracy: 0.95 \n",
            "Loss: 0.010118710808455944, Accuracy: 0.98 \n",
            "Loss: 0.007286061532795429, Accuracy: 0.99 \n",
            "Loss: 0.011651060543954372, Accuracy: 0.95 \n",
            "Loss: 0.015031901188194752, Accuracy: 0.94 \n",
            "Loss: 0.006809368263930082, Accuracy: 0.99 \n",
            "Loss: 0.009196311235427856, Accuracy: 0.97 \n",
            "Loss: 0.006235478911548853, Accuracy: 0.99 \n",
            "Loss: 0.008557108230888844, Accuracy: 0.98 \n",
            "Loss: 0.008111884817481041, Accuracy: 0.96 \n",
            "Loss: 0.009202990680932999, Accuracy: 0.98 \n",
            "Loss: 0.014561531133949757, Accuracy: 0.93 \n",
            "Loss: 0.005925353150814772, Accuracy: 0.98 \n",
            "Loss: 0.012940067797899246, Accuracy: 0.93 \n",
            "Loss: 0.013092845678329468, Accuracy: 0.95 \n",
            "Loss: 0.023348206654191017, Accuracy: 0.9 \n",
            "Loss: 0.010446366854012012, Accuracy: 0.97 \n",
            "Loss: 0.01656070165336132, Accuracy: 0.96 \n",
            "Loss: 0.013876566663384438, Accuracy: 0.97 \n",
            "Loss: 0.009258182719349861, Accuracy: 0.97 \n",
            "Loss: 0.027214879170060158, Accuracy: 0.93 \n",
            "Loss: 0.006262744776904583, Accuracy: 0.98 \n",
            "Loss: 0.00832701101899147, Accuracy: 0.98 \n",
            "Loss: 0.00934633705765009, Accuracy: 0.99 \n",
            "Loss: 0.01785486377775669, Accuracy: 0.94 \n",
            "Loss: 0.017637215554714203, Accuracy: 0.95 \n",
            "Loss: 0.01130703929811716, Accuracy: 0.96 \n",
            "Loss: 0.018501445651054382, Accuracy: 0.96 \n",
            "Loss: 0.013826839625835419, Accuracy: 0.96 \n",
            "Loss: 0.011578691191971302, Accuracy: 0.96 \n",
            "Loss: 0.005481523461639881, Accuracy: 0.99 \n",
            "Loss: 0.024344664067029953, Accuracy: 0.96 \n",
            "Loss: 0.018998753279447556, Accuracy: 0.92 \n",
            "Loss: 0.021207384765148163, Accuracy: 0.97 \n",
            "Loss: 0.007710822392255068, Accuracy: 0.98 \n",
            "Loss: 0.021150803193449974, Accuracy: 0.93 \n",
            "Loss: 0.013409949839115143, Accuracy: 0.94 \n",
            "Loss: 0.012898603454232216, Accuracy: 0.95 \n",
            "Loss: 0.011719572357833385, Accuracy: 0.95 \n",
            "Loss: 0.011334576644003391, Accuracy: 0.96 \n",
            "Loss: 0.00851381290704012, Accuracy: 0.97 \n",
            "Loss: 0.010782978497445583, Accuracy: 0.97 \n",
            "Loss: 0.01859349198639393, Accuracy: 0.96 \n",
            "Loss: 0.018560044467449188, Accuracy: 0.95 \n",
            "Loss: 0.025673694908618927, Accuracy: 0.92 \n",
            "Loss: 0.009371263906359673, Accuracy: 0.97 \n",
            "Loss: 0.005148771218955517, Accuracy: 0.99 \n",
            "Loss: 0.006391770206391811, Accuracy: 0.98 \n",
            "Loss: 0.005806686822324991, Accuracy: 1.0 \n",
            "Loss: 0.02368013560771942, Accuracy: 0.95 \n",
            "Loss: 0.007616809103637934, Accuracy: 0.98 \n",
            "Loss: 0.015848452225327492, Accuracy: 0.94 \n",
            "Loss: 0.01586165837943554, Accuracy: 0.95 \n",
            "Loss: 0.013108950108289719, Accuracy: 0.95 \n",
            "Loss: 0.014077126048505306, Accuracy: 0.96 \n",
            "Loss: 0.011493698693811893, Accuracy: 0.97 \n",
            "Loss: 0.014622208662331104, Accuracy: 0.96 \n",
            "Loss: 0.015400740318000317, Accuracy: 0.96 \n",
            "Loss: 0.017285717651247978, Accuracy: 0.96 \n",
            "Loss: 0.015531257726252079, Accuracy: 0.97 \n",
            "Loss: 0.008901847526431084, Accuracy: 0.97 \n",
            "Loss: 0.011296101845800877, Accuracy: 0.97 \n",
            "Loss: 0.013619070872664452, Accuracy: 0.98 \n",
            "Loss: 0.017977671697735786, Accuracy: 0.95 \n",
            "Loss: 0.006146435160189867, Accuracy: 1.0 \n",
            "Loss: 0.011880326084792614, Accuracy: 0.95 \n",
            "Loss: 0.012985722161829472, Accuracy: 0.95 \n",
            "Loss: 0.008269019424915314, Accuracy: 0.98 \n",
            "Loss: 0.009249260649085045, Accuracy: 0.98 \n",
            "Loss: 0.010149124078452587, Accuracy: 0.97 \n",
            "Loss: 0.011698869056999683, Accuracy: 0.97 \n",
            "Loss: 0.016182594001293182, Accuracy: 0.93 \n",
            "Loss: 0.0094212107360363, Accuracy: 0.96 \n",
            "Loss: 0.020993655547499657, Accuracy: 0.91 \n",
            "Loss: 0.012959016487002373, Accuracy: 0.96 \n",
            "Loss: 0.008004965260624886, Accuracy: 0.99 \n",
            "Loss: 0.01314918976277113, Accuracy: 0.98 \n",
            "Loss: 0.006786850281059742, Accuracy: 0.98 \n",
            "Loss: 0.009891591966152191, Accuracy: 0.98 \n",
            "Loss: 0.023424897342920303, Accuracy: 0.93 \n",
            "Loss: 0.016366539523005486, Accuracy: 0.94 \n",
            "Loss: 0.011456292122602463, Accuracy: 0.97 \n",
            "Loss: 0.006430258974432945, Accuracy: 0.99 \n",
            "Loss: 0.007762911729514599, Accuracy: 0.98 \n",
            "Loss: 0.009752687998116016, Accuracy: 0.98 \n",
            "Loss: 0.013255374506115913, Accuracy: 0.95 \n",
            "Loss: 0.010880652815103531, Accuracy: 0.98 \n",
            "Loss: 0.01644517481327057, Accuracy: 0.96 \n",
            "Loss: 0.020500274375081062, Accuracy: 0.92 \n",
            "Loss: 0.015095127746462822, Accuracy: 0.96 \n",
            "Loss: 0.004331780131906271, Accuracy: 0.99 \n",
            "Loss: 0.008146712556481361, Accuracy: 0.99 \n",
            "Loss: 0.014958771876990795, Accuracy: 0.96 \n",
            "Loss: 0.018288377672433853, Accuracy: 0.93 \n",
            "Loss: 0.01325526088476181, Accuracy: 0.97 \n",
            "Loss: 0.007732373662292957, Accuracy: 0.97 \n",
            "Loss: 0.008618702180683613, Accuracy: 0.99 \n",
            "Loss: 0.013554061762988567, Accuracy: 0.96 \n",
            "Loss: 0.019611431285738945, Accuracy: 0.95 \n",
            "Loss: 0.011970923282206059, Accuracy: 0.96 \n",
            "Loss: 0.013595884665846825, Accuracy: 0.95 \n",
            "Loss: 0.003944919910281897, Accuracy: 0.99 \n",
            "Loss: 0.019130757078528404, Accuracy: 0.91 \n",
            "Loss: 0.01285573747009039, Accuracy: 0.97 \n",
            "Loss: 0.01682283729314804, Accuracy: 0.96 \n",
            "Loss: 0.011809922754764557, Accuracy: 0.96 \n",
            "Loss: 0.014020557515323162, Accuracy: 0.97 \n",
            "Loss: 0.006334966514259577, Accuracy: 0.99 \n",
            "Loss: 0.011932127177715302, Accuracy: 0.95 \n",
            "Loss: 0.009505686350166798, Accuracy: 0.97 \n",
            "Loss: 0.0066308192908763885, Accuracy: 0.97 \n",
            "Loss: 0.008817292749881744, Accuracy: 0.98 \n",
            "Loss: 0.005670309066772461, Accuracy: 1.0 \n",
            "Loss: 0.009398631751537323, Accuracy: 0.98 \n",
            "Loss: 0.011358121410012245, Accuracy: 0.96 \n",
            "Loss: 0.007818842306733131, Accuracy: 0.98 \n",
            "Loss: 0.007672307081520557, Accuracy: 0.97 \n",
            "Loss: 0.00923836138099432, Accuracy: 0.98 \n",
            "Loss: 0.007988549768924713, Accuracy: 0.98 \n",
            "Loss: 0.006477646995335817, Accuracy: 0.97 \n",
            "Loss: 0.010894130915403366, Accuracy: 0.96 \n",
            "Loss: 0.010664884932339191, Accuracy: 0.96 \n",
            "Loss: 0.0038225234020501375, Accuracy: 1.0 \n",
            "Loss: 0.00908404216170311, Accuracy: 0.98 \n",
            "Loss: 0.011608677916228771, Accuracy: 0.95 \n",
            "Loss: 0.009253176860511303, Accuracy: 0.97 \n",
            "Loss: 0.02033168263733387, Accuracy: 0.92 \n",
            "Loss: 0.002608927898108959, Accuracy: 1.0 \n",
            "Loss: 0.015643546357750893, Accuracy: 0.94 \n",
            "Loss: 0.0062379417940974236, Accuracy: 0.98 \n",
            "Loss: 0.00964756403118372, Accuracy: 0.98 \n",
            "Loss: 0.015567424707114697, Accuracy: 0.98 \n",
            "Loss: 0.0103570856153965, Accuracy: 0.97 \n",
            "Loss: 0.005399273242801428, Accuracy: 0.98 \n",
            "Loss: 0.009308302775025368, Accuracy: 0.98 \n",
            "Loss: 0.012132901698350906, Accuracy: 0.98 \n",
            "Loss: 0.013081456534564495, Accuracy: 0.97 \n",
            "Loss: 0.01801958493888378, Accuracy: 0.95 \n",
            "Loss: 0.004850358236581087, Accuracy: 1.0 \n",
            "Loss: 0.010062107816338539, Accuracy: 0.98 \n",
            "Loss: 0.005947376601397991, Accuracy: 0.98 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLTWvCV79-YE"
      },
      "source": [
        "Wow so this all actually worked.  This is implementing a neural network FROM SCRATCH.  Pretty cool!!  I still have a LONG way to go though, something as \"simple\" as this had me absolutely stumped for so long...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIDpvtWYv4Kh",
        "outputId": "855fe6e9-1451-4a21-f220-97e810b002c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(accuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5dd4509eb8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1zivB7Bk7nq"
      },
      "source": [
        "Lets test it on the test set.  Starting with just one image, then extending to all of the test set and checking accuracy on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHeWwrdNk_Aa",
        "outputId": "36c4888d-e529-4de6-bcdd-f0a4d910dfd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "\n",
        "\n",
        "x = x_test[0].reshape(-1,28*28)\n",
        "y=y_test[0]\n",
        "\n",
        "plt.imshow(x_test[0])\n",
        "print(\"Actual Label: {}\".format(y_test[0]))\n",
        "x = x.dot(layer1)\n",
        "x = np.maximum(x,0)\n",
        "x = x.dot(layer2)\n",
        "print(\"Predicted Label: {}\".format(np.argmax(x)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Label: 7\n",
            "Predicted Label: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwAzq6T2oi8O",
        "outputId": "be409e47-dc90-4371-933f-d93d26344d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = x_test.reshape(-1,28*28)\n",
        "y = y_test\n",
        "\n",
        "#Quick forward pass using current weights\n",
        "x = x.dot(layer1) \n",
        "x = np.maximum(x,0)\n",
        "x = x.dot(layer2)\n",
        "\n",
        "acc = (y_test == np.argmax(x,axis=1)).mean()\n",
        "\n",
        "print(\"Accuracy on test set: {}\".format(acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.9601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DER9A4JUprob"
      },
      "source": [
        "So we get 96% accuracy on the test set.  Cooool stuff."
      ]
    }
  ]
}