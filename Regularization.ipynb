{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTK+/iIqXd1VVt4j8xpmQy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fidem/PortfolioProjects/blob/main/Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui5QVSkGDr8y"
      },
      "source": [
        "#First off, a Bias vs Variance recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH4PJeILDw_2"
      },
      "source": [
        "Bias stems from wrong assumptions in the learning algorithm.  High bias may cause the algorithm to miss relevant relations between features and target outputs.  AKA - Imagine data that is curved, and fitting a linear regression line to this data.  This line wont be able to capture the curved nature of the data, and therefore cause high bias in the model - ALSO KNOWN AS UNDERFITTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDjjNHSfFepl"
      },
      "source": [
        "Variance stems from having too much sensitivity to small fluctuations in the training set.  This means fitting an over-complicated model to data, which performs extremely well on the training set, and usually abysmally on the testing set.  This is high variance, also known as overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfVxpfjvFxBv"
      },
      "source": [
        "There is a tradeoff between bias and variance, and we want to find a way to find the \"sweet spot\", minimising both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkMEnh9_F9XF"
      },
      "source": [
        "#L1 Regularization - LASSO REGRESSION!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhee_bfYGGz2"
      },
      "source": [
        "So when Linear Regression determines the values for its line, it minimises the sum of the squared residuals.  However, when Lasso Regression determines the values for its line, it minimises:\n",
        "# The Sum of the Squared Residuals + $\\lambda\\cdot$| The Slope |\n",
        "where $\\lambda$ is determined through cross validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJMlD6jII_97"
      },
      "source": [
        "#L2 Regularization - Ridge Regression!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVUdyJ-MJEus"
      },
      "source": [
        "When ridge regression determines the values for its line, it minimises:\n",
        "# The Sum of the Squared Residuals + $\\lambda \\cdot$ The Slope$^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txCZncmiJgBH"
      },
      "source": [
        "The differences????\n",
        "\n",
        "Well, Lasso regularization can shrink the slope ALL the way to 0 but Ridge regression can only shrink the slope asymptotically close to 0.\n",
        "\n",
        "So, as you increase $\\lambda$, in Lasso regression, parameters can shrink ALL the way to 0 (aka dont affect the model at all), but in ridge regression, parameters cannot shrink all the way to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWwA7RXGKMVs"
      },
      "source": [
        "Ridge Regression tends to do better when all the variables are useful\n",
        "\n",
        "Lasso Regression tends to do better when there are useless variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ9N241GKMPD"
      },
      "source": [
        "#L1 and L2 Regularization... Cost Function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX5UeBFdKpj4"
      },
      "source": [
        "More generally, A regression model that uses L1 regularization is known as LASSO regression, and if it uses L2 regularization it is known as Ridge Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyTe9Tx5K8LC"
      },
      "source": [
        "Ridge Regression adds the SQUARED MAGNITUDE of the coefficients as a penalty term to the loss function.\n",
        "# $\\sum_i{(y_i - \\sum_j{x_{ij}\\beta_j})^2} + \\lambda \\sum_j{\\beta_j^2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G_ptcUtLz3A"
      },
      "source": [
        "Lasso Regression adds the ABSOLUTE VALUE of MAGNITUDE of the coefficients as a penalty term:\n",
        "# $\\sum_i{(y_i - \\sum_j{x_{ij}\\beta_j})^2} + \\lambda \\sum_j{|{\\beta_j}|}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YnJU2dZMKC-"
      },
      "source": [
        "And lastly to reiterate, the KEY DIFFERENCE between the two is that L1 Regularization (LASSO) shrinks the less important features coefficients to 0, effectively getting rid of these features.  L2 Regularization (Ridge) can only shrink very close to 0, never effectively getting rid of features."
      ]
    }
  ]
}