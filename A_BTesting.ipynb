{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A/BTesting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTPrpRBvushdvoK3OAdOT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fidem/PortfolioProjects/blob/main/A_BTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg-7HGRpESpz"
      },
      "source": [
        "#A/B Testing - My notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sNJO4V7wnFr"
      },
      "source": [
        "A/B testing is a general methodology used online when you want to test a new product or feature.  You take 2 sets of users, and show one set (the control set) the existing product, and show the other set (your experiment set) the new version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCXiW5gCxhS1"
      },
      "source": [
        "You can test all kinds of things.  Examples are website UI changes, personalised ads, website load speed, search engine ranking changes etc etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05cYvPaT16kn"
      },
      "source": [
        "So first we need to design the experiment, and formulate the hypothesis.  The example I will run through is:\n",
        "#Hypothesis: Changing \"Register\" button on website from 100x100 pixels to 300x300 pixels will increase the amount of accounts created.\n",
        "\n",
        "And we define some metric to measure that will represent this.  In this case, we can simply use a probability of (Users that create an account) / (Users that visit the site).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6iS1rC13IdR"
      },
      "source": [
        "This metric is a \"proportion of success\" metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLeNf-HN3T_d"
      },
      "source": [
        "Next we need to decide on what distribution can best represent this metric.  In this case, the outcome is either (success) or (failure).  And this binary \n",
        "response can be represented using a binomial distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGNrPhY83f2Z"
      },
      "source": [
        "#Calculating Confidence Intervals\n",
        "CIs can be used to determine what results surprise us, and therefore prove the hypothesis.  \n",
        "First, we need to calculate our estimated probability from the control group.  This is given by \n",
        "# $\\hat{P} = \\frac{Users\\:Who\\:Created\\:Accounts}{Users\\:who\\:visited\\:site}$ = $\\frac{X}{N}$\n",
        "\n",
        "This $\\hat{P}$ is the centre of our confidence interval.  Next is to find the upper and lower limits.  In most cases, we want 95% confidence intervals.  To do this we need to find the margin error, given by:\n",
        "\n",
        "# $Z_{score} * SE$\n",
        "\n",
        "Use a Z score table to look up the Z score.  Essentially find the confidence interval you want.  In the case of a 95% CI two tailed test, you want confidence intervals of 2.5% and 97.5%.  In the z score table look up 0.0250 and 0.9750.  The resultant Z scores should be -1.96 and 1.96.\n",
        "Then used the SE defined by the distribution you are using to calculate the margins.  \n",
        "In this case we use binomial with probability of success.\n",
        "# $SE = \\sqrt{\\frac{\\hat{P}(1-\\hat{P})}{N}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNjQMs1J8PkT"
      },
      "source": [
        "So now we have the confidence intervals of the control set click-through rate.  Now we actually use a hypothesis test!!!\n",
        "# $H_0:$ No difference in create account probability between control group and experiment group\n",
        "# $H_1:$ There is a difference in create account probability between control group and experiment group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSwgGVdw9SE7"
      },
      "source": [
        "As aside on two tailed tests vs one tailed tests.  Two tailed tests determine if the alternative hypothesis is significantly more positive OR negative.  One tailed tests only check in one direction, and this is usually positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc7F-Zg_9wSH"
      },
      "source": [
        "So we are comparing TWO samples.  So we need some standard error that can represent both of them.  To calculate this we use Pooled Standard Error.\n",
        "\n",
        "# $\\hat{P}_{pooled} = \\frac{X_{con} + X_{exp}}{N_{con} + N_{exp}}$\n",
        "\n",
        "# $SE_{pooled} = \\sqrt{\\frac{\\hat{P}_{pooled}(1-\\hat{P}_{pooled})}{N_{con} + N_{exp}}}$\n",
        "\n",
        "Then using the difference between the probabilities of each sample:\n",
        "# $\\hat{d} = \\hat{P}_{exp} - \\hat{P}_{con} $\n",
        "\n",
        "We can refine the null hypothesis to:\n",
        "# $H_0: \\hat{d} = 0 \\:\\:\\:\\:\\:\\:\\:\\: \\hat{d} \\sim N(0, SE_{pooled})$\n",
        "\n",
        "Then\n",
        "\n",
        "#if $\\hat{d} > 1.96*SE_{pooled} \\;\\:or\\;\\; \\hat{d} < -1.96*SE_{pooled}$:\n",
        "we reject the null hypothesis, and conclude that the experiment did lead to a significantly different account creation probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6dK9tVHCZ9c"
      },
      "source": [
        "The next part is really thinking about is this change a practical change.  More business side decisions on whether its worth to do this change. \n",
        "So we can say that from a business perspective, a 2% change in account creation probability would be a practically significant change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vk98HwODPyR"
      },
      "source": [
        "Size vs Power tradeoff:\n",
        "\"How many user visits do we need in order to get a statistically significant result\". - Statistical Power\n",
        "Power has an inverse trade-off with size.  You want more power, but to get more power you need to increase sample size.  And sample size can be difficult to increase in most cases."
      ]
    }
  ]
}